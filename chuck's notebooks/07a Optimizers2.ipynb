{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#SGD\" data-toc-modified-id=\"SGD-0.1\"><span class=\"toc-item-num\">0.1&nbsp;&nbsp;</span>SGD</a></span></li><li><span><a href=\"#AdamW\" data-toc-modified-id=\"AdamW-0.2\"><span class=\"toc-item-num\">0.2&nbsp;&nbsp;</span>AdamW</a></span></li><li><span><a href=\"#SCG\" data-toc-modified-id=\"SCG-0.3\"><span class=\"toc-item-num\">0.3&nbsp;&nbsp;</span>SCG</a></span></li></ul></li><li><span><a href=\"#Demonstration-of-Training-Weights-for-Neural-Network-with-Single-Hidden-Layer\" data-toc-modified-id=\"Demonstration-of-Training-Weights-for-Neural-Network-with-Single-Hidden-Layer-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Demonstration of Training Weights for Neural Network with Single Hidden Layer</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand{\\xv}{\\mathbf{x}}\n",
    "\\newcommand{\\Xv}{\\mathbf{X}}\n",
    "\\newcommand{\\yv}{\\mathbf{y}}\n",
    "\\newcommand{\\Yv}{\\mathbf{Y}}\n",
    "\\newcommand{\\zv}{\\mathbf{z}}\n",
    "\\newcommand{\\av}{\\mathbf{a}}\n",
    "\\newcommand{\\Wv}{\\mathbf{W}}\n",
    "\\newcommand{\\wv}{\\mathbf{w}}\n",
    "\\newcommand{\\gv}{\\mathbf{g}}\n",
    "\\newcommand{\\Hv}{\\mathbf{H}}\n",
    "\\newcommand{\\dv}{\\mathbf{d}}\n",
    "\\newcommand{\\Vv}{\\mathbf{V}}\n",
    "\\newcommand{\\vv}{\\mathbf{v}}\n",
    "\\newcommand{\\tv}{\\mathbf{t}}\n",
    "\\newcommand{\\Tv}{\\mathbf{T}}\n",
    "\\newcommand{\\zv}{\\mathbf{z}}\n",
    "\\newcommand{\\Zv}{\\mathbf{Z}}\n",
    "\\newcommand{\\muv}{\\boldsymbol{\\mu}}\n",
    "\\newcommand{\\sigmav}{\\boldsymbol{\\sigma}}\n",
    "\\newcommand{\\phiv}{\\boldsymbol{\\phi}}\n",
    "\\newcommand{\\Phiv}{\\boldsymbol{\\Phi}}\n",
    "\\newcommand{\\Sigmav}{\\boldsymbol{\\Sigma}}\n",
    "\\newcommand{\\Lambdav}{\\boldsymbol{\\Lambda}}\n",
    "\\newcommand{\\half}{\\frac{1}{2}}\n",
    "\\newcommand{\\argmax}[1]{\\underset{#1}{\\operatorname{argmax}}}\n",
    "\\newcommand{\\argmin}[1]{\\underset{#1}{\\operatorname{argmin}}}\n",
    "\\newcommand{\\dimensionbar}[1]{\\underset{#1}{\\operatorname{|}}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"8\">**Optimizers2**</font> \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-27T18:55:13.547190Z",
     "start_time": "2022-09-27T18:55:13.542106Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing optimizers2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile optimizers2.py\n",
    "\n",
    "import numpy as np\n",
    "import copy\n",
    "import math\n",
    "import sys  # for sys.float_info.epsilon\n",
    "\n",
    "######################################################################\n",
    "## class Optimizers()\n",
    "######################################################################\n",
    "\n",
    "class Optimizers():\n",
    "\n",
    "    def __init__(self, all_weights):\n",
    "        '''all_weights is a vector of all of a neural networks weights concatenated into a one-dimensional vector'''\n",
    "        \n",
    "        self.all_weights = all_weights\n",
    "        self.error_trace = []\n",
    "        self.best_val_error = None\n",
    "        self.best_epoch = None\n",
    "        self.best_weights = None "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD\n",
    "<a id='sgd'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the first optimization algorithm, SGD.  We include the option of using [Nesterov accelerated gradient (NAG)](https://ruder.io/optimizing-gradient-descent/index.html#nesterovacceleratedgradient).  This link is a nice summary of many other optimizers, also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-27T18:55:13.557375Z",
     "start_time": "2022-09-27T18:55:13.548683Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to optimizers2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a optimizers2.py\n",
    "\n",
    "    def sgd(self, Xtrain, Ttrain, Xval, Tval, error_f, gradient_f, n_epochs=100, learning_rate=0.001, momentum=0, \n",
    "            weight_penalty=0,\n",
    "            error_convert_f=None, error_convert_name='MSE', nesterov=False, verbose=True):\n",
    "        '''\n",
    "        Xtrain : two-dimensional numpy array \n",
    "            number of training samples  by  number of input components\n",
    "        Ttrain : two-dimensional numpy array\n",
    "            number of training samples  by  number of output components\n",
    "        Xvalidate : two-dimensional numpy array \n",
    "            number of validation samples  by  number of input components\n",
    "        Tvalidate : two-dimensional numpy array\n",
    "            number of validationg samples  by  number of output components\n",
    "        error_f: function that requires X and T as arguments (given in fargs) and returns mean squared error.\n",
    "        gradient_f: function that requires X and T as arguments (in fargs) and returns gradient of mean squared error\n",
    "                    with respect to each weight.\n",
    "        n_epochs : int\n",
    "            Number of passes to take through all samples\n",
    "        learning_rate : float\n",
    "            Controls the step size of each update, only for sgd and adam\n",
    "        momentum : float\n",
    "            Controls amount of previous weight update to add to current weight update, only for sgd\n",
    "        weight_penalty : float\n",
    "            Controls amount to penalize large magnitude weights\n",
    "        error_convert_f: function that converts the standardized error from error_f to original T units\n",
    "        error_convert_name: used when printing progress updates\n",
    "        nestoerov: momentum factor for Nesterov momentum. If False, then uses zero for no momentum\n",
    "        verbose: if True print progress occasionally\n",
    "        '''\n",
    "\n",
    "        if self.error_trace == []:\n",
    "            # not initialized yet\n",
    "            self.prev_update = 0\n",
    "            if nesterov:\n",
    "                self.all_weights_copy = np.zeros(self.all_weights.shape)\n",
    " \n",
    "        # learning_rate /= (Xtrain.shape[0] * Ttrain.shape[1])\n",
    "        \n",
    "        epochs_per_print = n_epochs // 10\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "\n",
    "            # Assume to be standardized already\n",
    "            error = error_f(Xtrain, Ttrain) + weight_penalty * np.sum(self.all_weights ** 2)\n",
    "            grad = gradient_f(Xtrain, Ttrain) + weight_penalty * self.all_weights\n",
    "\n",
    "            val_error = error_f(Xval, Tval) + weight_penalty * np.sum(self.all_weights ** 2)\n",
    "\n",
    "            if not nesterov:\n",
    "                \n",
    "                self.prev_update = learning_rate * grad + momentum * self.prev_update\n",
    "                # Update all weights using -= to modify their values in-place.\n",
    "                self.all_weights -= self.prev_update\n",
    "\n",
    "            else:\n",
    "\n",
    "                self.all_weights_copy[:] = self.all_weights \n",
    "                self.all_weights -= momentum * self.prev_update\n",
    "                error = error_f(Xtrain, Ttrain)\n",
    "                grad = gradient_f(Xtrain, Ttrain)\n",
    "                self.prev_update = learning_rate * grad + momentum * self.prev_update\n",
    "                self.all_weights[:] = self.all_weights_copy\n",
    "                self.all_weights -= self.prev_update\n",
    "                \n",
    "\n",
    "            if error_convert_f:\n",
    "                error = error_convert_f(error)\n",
    "                val_error = error_convert_f(val_error)\n",
    "\n",
    "            self.error_trace.append([error, val_error])\n",
    "\n",
    "            if verbose and ((epoch + 1) % max(1, epochs_per_print) == 0):\n",
    "                print(f'SGD: Epoch {epoch + 1} {error_convert_name}={error:.5f},{val_error:.5f}')\n",
    "\n",
    "            if self.best_val_error is None or val_error < self.best_val_error:\n",
    "                self.best_val_error = val_error\n",
    "                self.best_epoch = epoch\n",
    "                self.best_weights = self.all_weights.copy()\n",
    "\n",
    "        self.all_weights[:] = self.best_weights\n",
    "\n",
    "        return self.error_trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdamW\n",
    "<a id='adamw'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we add the basic [Adaptive Moment Estimation (Adam)](https://ruder.io/optimizing-gradient-descent/index.html#adam).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-27T18:55:13.567042Z",
     "start_time": "2022-09-27T18:55:13.558837Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to optimizers2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a optimizers2.py\n",
    "\n",
    "    ######################################################################\n",
    "    #### adamw\n",
    "    ######################################################################\n",
    "\n",
    "    def adamw(self, Xtrain, Ttrain, Xval, Tval, error_f, gradient_f, n_epochs=100, learning_rate=0.001, weight_penalty=0,\n",
    "              error_convert_f=None, error_convert_name='MSE', verbose=True):\n",
    "        '''\n",
    "        Xtrain : two-dimensional numpy array \n",
    "            number of training samples  by  number of input components\n",
    "        Ttrain : two-dimensional numpy array\n",
    "            number of training samples  by  number of output components\n",
    "        Xvalidate : two-dimensional numpy array \n",
    "            number of validation samples  by  number of input components\n",
    "        Tvalidate : two-dimensional numpy array\n",
    "            number of validationg samples  by  number of output components\n",
    "        error_f: function that requires X and T as arguments (given in fargs) and returns mean squared error.\n",
    "        gradient_f: function that requires X and T as arguments (in fargs) and returns gradient of mean squared error\n",
    "                    with respect to each weight.\n",
    "        n_epochs : int\n",
    "            Number of passes to take through all samples\n",
    "        learning_rate : float\n",
    "            Controls the step size of each update, only for sgd and adam\n",
    "        weight_penalty : float\n",
    "            Controls amount to penalize large magnitude weights\n",
    "        error_convert_f: function that converts the standardized error from error_f to original T units\n",
    "        error_convert_name: used when printing progress updates\n",
    "        weight_penalty: if > 0, penalize large magnitude weights\n",
    "        verbose: if True print progress occasionally\n",
    "        '''\n",
    "\n",
    "        if self.error_trace == []:\n",
    "            shape = self.all_weights.shape\n",
    "            # with multiple subsets (batches) of training data.\n",
    "            self.mt = np.zeros(shape)\n",
    "            self.vt = np.zeros(shape)\n",
    "            self.sqrt = np.sqrt\n",
    "                \n",
    "            self.beta1 = 0.9\n",
    "            self.beta2 = 0.999\n",
    "            self.beta1t = 1\n",
    "            self.beta2t = 1\n",
    "\n",
    "        # learning_rate /= (Xtrain.shape[0] * Ttrain.shape[1])\n",
    "\n",
    "        alpha = learning_rate  # learning rate called alpha in original paper on adam\n",
    "        epsilon = 1e-8\n",
    "        epochs_per_print = n_epochs // 10\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "\n",
    "            error = error_f(Xtrain, Ttrain) + weight_penalty * np.sum(self.all_weights ** 2)\n",
    "            grad = gradient_f(Xtrain, Ttrain) + weight_penalty * self.all_weights\n",
    "\n",
    "            val_error = error_f(Xval, Tval) + weight_penalty * np.sum(self.all_weights ** 2)\n",
    "\n",
    "            self.mt[:] = self.beta1 * self.mt + (1 - self.beta1) * grad\n",
    "            self.vt[:] = self.beta2 * self.vt + (1 - self.beta2) * grad * grad\n",
    "            self.beta1t *= self.beta1\n",
    "            self.beta2t *= self.beta2\n",
    "\n",
    "            m_hat = self.mt / (1 - self.beta1t)\n",
    "            v_hat = self.vt / (1 - self.beta2t)\n",
    "\n",
    "            # Update all weights using -= to modify their values in-place.\n",
    "            self.all_weights -= (alpha * m_hat / (self.sqrt(v_hat) + epsilon) + \n",
    "                                 weight_penalty * self.all_weights)\n",
    "    \n",
    "            if error_convert_f:\n",
    "                error = error_convert_f(error)\n",
    "                val_error = error_convert_f(val_error)\n",
    "\n",
    "            self.error_trace.append([error, val_error])\n",
    "\n",
    "            if verbose and ((epoch + 1) % max(1, epochs_per_print) == 0):\n",
    "                print(f'AdamW: Epoch {epoch + 1} {error_convert_name}={error:.5f},{val_error:.5f}')\n",
    "\n",
    "            if self.best_val_error is None or val_error < self.best_val_error:\n",
    "                self.best_val_error = val_error\n",
    "                self.best_epoch = epoch\n",
    "                self.best_weights = self.all_weights.copy()\n",
    "                \n",
    "        self.all_weights[:] = self.best_weights\n",
    "\n",
    "        return self.error_trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCG \n",
    "<a id='scg'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now the Scaled Conjugate Gradient, scg, algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-27T18:55:13.577329Z",
     "start_time": "2022-09-27T18:55:13.568417Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to optimizers2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a optimizers2.py\n",
    "\n",
    "    ######################################################################\n",
    "    #### scg\n",
    "    ######################################################################\n",
    "\n",
    "    def scg(self, Xtrain, Ttrain, Xval, Tval, error_f, gradient_f, n_epochs=100, weight_penalty=0,\n",
    "            error_convert_f=None, error_convert_name='MSE', verbose=True):\n",
    "        '''\n",
    "        Xtrain : two-dimensional numpy array \n",
    "            number of training samples  by  number of input components\n",
    "        Ttrain : two-dimensional numpy array\n",
    "            number of training samples  by  number of output components\n",
    "        Xvalidate : two-dimensional numpy array \n",
    "            number of validation samples  by  number of input components\n",
    "        Tvalidate : two-dimensional numpy array\n",
    "            number of validationg samples  by  number of output components\n",
    "        error_f: function that requires X and T as arguments (given in fargs) and returns mean squared error.\n",
    "        gradient_f: function that requires X and T as arguments (in fargs) and returns gradient of mean squared error\n",
    "                    with respect to each weight.\n",
    "        n_epochs : int\n",
    "            Number of passes to take through all samples\n",
    "        weight_penalty : float\n",
    "            Controls amount to penalize large magnitude weights\n",
    "        error_convert_f: function that converts the standardized error from error_f to original T units\n",
    "        error_convert_name: used when printing progress updates\n",
    "        weight_penalty: if > 0, penalize large magnitude weights\n",
    "        verbose: if True print progress occasionally\n",
    "        '''\n",
    "\n",
    "        if self.error_trace == []:\n",
    "            shape = self.all_weights.shape\n",
    "            self.w_new = np.zeros(shape)\n",
    "            self.w_temp = np.zeros(shape)\n",
    "            self.g_new = np.zeros(shape)\n",
    "            self.g_old = np.zeros(shape)\n",
    "            self.g_smallstep = np.zeros(shape)\n",
    "            self.search_dir = np.zeros(shape)\n",
    "\n",
    "        sigma0 = 1.0e-6\n",
    "        val_fold = error_f(Xval, Tval) + weight_penalty * np.sum(self.all_weights ** 2)\n",
    "        fold = error_f(Xtrain, Ttrain) + weight_penalty * np.sum(self.all_weights ** 2)\n",
    "        error = fold\n",
    "        val_error = val_fold\n",
    "        self.g_new[:] = gradient_f(Xtrain, Ttrain) + weight_penalty * self.all_weights\n",
    "        self.g_old[:] = copy.deepcopy(self.g_new)\n",
    "        self.search_dir[:] = -self.g_new\n",
    "        success = True\t\t\t\t# Force calculation of directional derivs.\n",
    "        nsuccess = 0\t\t\t\t# nsuccess counts number of successes.\n",
    "        beta = 1.0e-6\t\t\t\t# Initial scale parameter. Lambda in Moeller.\n",
    "        betamin = 1.0e-15 \t\t\t# Lower bound on scale.\n",
    "        betamax = 1.0e20\t\t\t# Upper bound on scale.\n",
    "        nvars = len(self.all_weights)\n",
    "        epoch = 1\t\t\t\t# j counts number of epochs\n",
    "\n",
    "        # Main optimization loop.\n",
    "        while epoch <= n_epochs:\n",
    "\n",
    "            # Calculate first and second directional derivatives.\n",
    "            if success:\n",
    "                mu = self.search_dir @ self.g_new\n",
    "                if mu >= 0:\n",
    "                    self.search_dir[:] = - self.g_new\n",
    "                    mu = self.search_dir.T @ self.g_new\n",
    "                kappa = self.search_dir.T @ self.search_dir\n",
    "                if math.isnan(kappa):\n",
    "                    print('kappa', kappa)\n",
    "\n",
    "                if kappa < sys.float_info.epsilon:\n",
    "                    return self.error_trace\n",
    "\n",
    "                sigma = sigma0 / math.sqrt(kappa)\n",
    "\n",
    "                self.w_temp[:] = self.all_weights\n",
    "                self.all_weights += sigma * self.search_dir\n",
    "                error_f(Xtrain, Ttrain)  #  + weight_penalty * np.sum(self.all_weights ** 2)\n",
    "                self.g_smallstep[:] = gradient_f(Xtrain, Ttrain) + weight_penalty * self.all_weights\n",
    "                self.all_weights[:] = self.w_temp\n",
    "\n",
    "                theta = self.search_dir @ (self.g_smallstep - self.g_new) / sigma\n",
    "                if math.isnan(theta):\n",
    "                    print('theta', theta, 'sigma', sigma, 'search_dir[0]', self.search_dir[0], 'g_smallstep[0]', self.g_smallstep[0])\n",
    "\n",
    "            ## Increase effective curvature and evaluate step size alpha.\n",
    "\n",
    "            delta = theta + beta * kappa\n",
    "            # if math.isnan(scalarv(delta)):\n",
    "            if math.isnan(delta):\n",
    "                print('delta is NaN', 'theta', theta, 'beta', beta, 'kappa', kappa)\n",
    "            elif delta <= 0:\n",
    "                delta = beta * kappa\n",
    "                beta = beta - theta / kappa\n",
    "\n",
    "            if delta == 0:\n",
    "                success = False\n",
    "                fnow = fold\n",
    "                val_fnow = val_fold\n",
    "            else:\n",
    "                alpha = -mu / delta\n",
    "                ## Calculate the comparison ratio Delta\n",
    "                self.w_temp[:] = self.all_weights\n",
    "                self.all_weights += alpha * self.search_dir\n",
    "                val_fnew = error_f(Xval, Tval) + weight_penalty * np.sum(self.all_weights ** 2)\n",
    "                fnew = error_f(Xtrain, Ttrain) + weight_penalty * np.sum(self.all_weights ** 2)\n",
    "                Delta = 2 * (fnew - fold) / (alpha * mu)\n",
    "                if not math.isnan(Delta) and Delta  >= 0:\n",
    "                    success = True\n",
    "                    nsuccess += 1\n",
    "                    fnow = fnew\n",
    "                    val_fnow = val_fnew\n",
    "                else:\n",
    "                    success = False\n",
    "                    fnow = fold\n",
    "                    val_fnow = val_fold\n",
    "                    self.all_weights[:] = self.w_temp\n",
    "\n",
    "            epochsPerPrint = math.ceil(n_epochs/10)\n",
    "            if verbose and epoch % max(1, epochsPerPrint) == 0:\n",
    "                print(f'SCG: Epoch {epoch} {error_convert_name}={error:.5f},{val_error:.5f}')\n",
    "\n",
    "            if error_convert_f:\n",
    "                val_error = error_convert_f(val_fnow)\n",
    "                error = error_convert_f(fnow)\n",
    "            else:\n",
    "                error = fnow\n",
    "                val_error = val_fnow\n",
    "            self.error_trace.append([error, val_error])\n",
    "\n",
    "            if success:\n",
    "\n",
    "                fold = fnew\n",
    "                val_fold = val_fnew\n",
    "                self.g_old[:] = self.g_new\n",
    "                self.g_new[:] = gradient_f(Xtrain, Ttrain) + weight_penalty * self.all_weights\n",
    "\n",
    "                # If the gradient is zero then we are done.\n",
    "                gg = self.g_new @ self.g_new  # dot(gradnew, gradnew)\n",
    "                if gg == 0:\n",
    "                    return self.error_trace\n",
    "\n",
    "            if math.isnan(Delta) or Delta < 0.25:\n",
    "                beta = min(4.0 * beta, betamax)\n",
    "            elif Delta > 0.75:\n",
    "                beta = max(0.5 * beta, betamin)\n",
    "\n",
    "            # Update search direction using Polak-Ribiere formula, or re-start\n",
    "            # in direction of negative gradient after nparams steps.\n",
    "            if nsuccess == nvars:\n",
    "                self.search_dir[:] = -self.g_new\n",
    "                nsuccess = 0\n",
    "            elif success:\n",
    "                gamma = (self.g_old - self.g_new) @ (self.g_new / mu)\n",
    "                #self.search_dir[:] = gamma * self.search_dir - self.g_new\n",
    "                self.search_dir *= gamma\n",
    "                self.search_dir -= self.g_new\n",
    "\n",
    "            epoch += 1\n",
    "\n",
    "            # If we get here, then we haven't terminated in the given number of\n",
    "            # epochs.\n",
    "\n",
    "            if self.best_val_error is None or val_error < self.best_val_error:\n",
    "                self.best_val_error = val_error\n",
    "                self.best_epoch = epoch\n",
    "                self.best_weights = self.all_weights.copy()\n",
    "\n",
    "        self.all_weights[:] = self.best_weights\n",
    "\n",
    "        return self.error_trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstration of Training Weights for Neural Network with Single Hidden Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import optimizers2 as opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# Make date and partition randomly into training, validation, and testing\n",
    "\n",
    "Xtrain = np.arange(-2, 2, 0.05).reshape(-1, 1)\n",
    "Ttrain = np.sin(Xtrain) * np.sin(Xtrain * 5)\n",
    "    \n",
    "Xval = Xtrain * 1.1  # + 0.2\n",
    "Tval = Ttrain + 0.2 * Xtrain\n",
    "Xtest = Xtrain * 0.97\n",
    "Ttest = Ttrain + 0.15 * Xtrain # + np.random.uniform(-0.05, 0.05, Ttrain.shape) \n",
    "        \n",
    "n_samples, n_inputs = Xtrain.shape\n",
    "n_outputs = Ttrain.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0. ],\n",
       "        [0.5],\n",
       "        [1. ],\n",
       "        [1.5]]),\n",
       " array([[ 0.        ],\n",
       "        [-0.45973279],\n",
       "        [-0.45777798],\n",
       "        [ 0.64865886]]))"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.arange(0, 2, 0.5).reshape(-1, 1)\n",
    "T = np.sin(X) * np.sin(X * 10)\n",
    "n_samples, n_inputs = X.shape\n",
    "n_outputs = 1\n",
    "\n",
    "X,T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ttrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# Make weight and gradient one-dimensional vectors and matrix views\n",
    "# Neural net has only one hidden layer.\n",
    "# first Layer\n",
    "\n",
    "n_hiddens_1 = 2\n",
    "n_hiddens_2 = 2\n",
    "\n",
    "# Weight vector and matrix views\n",
    "    \n",
    "V0shape = (1 + n_inputs, n_hiddens_1)\n",
    "V1shape = (1+n_hiddens_1, n_hiddens_2)\n",
    "Wshape = (1 + n_hiddens_2, n_outputs)\n",
    "\n",
    "n_V0 = np.prod(V0shape)\n",
    "n_V1 = np.prod(V1shape)\n",
    "n_W = np.prod(Wshape)\n",
    "n_weights = n_V0 +n_V1 + n_W\n",
    "all_weights = np.random.uniform(-1, 1, n_weights)\n",
    "V0 = all_weights[:n_V0].reshape(V0shape)\n",
    "V0 /= np.sqrt(V0shape[0])\n",
    "V1 = all_weights[n_V0:n_V1+n_V0].reshape(V1shape)\n",
    "V1 /= np.sqrt(V1shape[0])\n",
    "W = all_weights[n_V1+n_V0:].reshape(Wshape)\n",
    "W /= np.sqrt(Wshape[0])\n",
    "Ws = [V0, V1, W]\n",
    "\n",
    "# Gradient vector and matrix views\n",
    "\n",
    "all_gradients = np.zeros_like(all_weights)\n",
    "grad_V0 = all_gradients[:n_V0].reshape(V0shape)\n",
    "grad_V1 = all_gradients[n_V0:n_V1+n_V0].reshape(V1shape)\n",
    "grad_W = all_gradients[n_V1+n_V0:].reshape(Wshape) \n",
    "\n",
    "grads = [grad_V0, grad_V1, grad_W]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13,)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 2), (3, 2), (3, 1))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V0.shape, V1.shape, W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 2), (3, 2), (3, 1))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V0shape, V1shape, Wshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13,)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_gradients.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13,)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 1)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# Neural network functions\n",
    "    \n",
    "def rmse_unstandardized(T, Y, Tstds):\n",
    "    return np.sqrt(np.mean((T - Y)**2, axis=0))[0]\n",
    "\n",
    "def add_ones(A):\n",
    "    return np.insert(A, 0, 1, axis=1)\n",
    "\n",
    "# Function to be minimized by optimizer method, mean squared error\n",
    "def error_f(X, T):\n",
    "    global Zs, Z0, Z1, Y\n",
    "    # forward pass\n",
    "    Z0 = np.tanh(add_ones(X) @ V0)\n",
    "    Z1 = np.tanh(add_ones(Z0) @ V1)\n",
    "    Y = add_ones(Z1) @ W\n",
    "    mean_sq_error = np.mean((T - Y) ** 2)\n",
    "    \n",
    "    Zs = [X,Z0, Z1, Y]     \n",
    "    print(Z0, Z1, Y)\n",
    "#     for i in range(len(self.Ws) -1):\n",
    "#         z = np.tanh(add_ones(self.Zs[i]) @ self.Ws[i])\n",
    "#         self.Zs.append(z)\n",
    "\n",
    "#     Y = add_ones(Zs[-1]) @ Ws[-1]\n",
    "#     Zs.append(Y)\n",
    "    return mean_sq_error\n",
    "\n",
    "# Gradient of function to be minimized for use by optimizer method\n",
    "def gradient_f(X, T):\n",
    "    global Zs, Z0, Z1, Y, grad_W, grad_V0, grad_V1, all_gradients   # remove ones that aren't changed in this function\n",
    "    n_samples = X.shape[0]\n",
    "    n_outputs = T.shape[1]\n",
    "    Dw = -(T - Y) / (n_samples * n_outputs)\n",
    "    grad_W[:] = add_ones(Z1).T @ Dw\n",
    "    \n",
    "    Dv1 = Dw @ W[1:, :].T * (1 - Z1**2)\n",
    "    grad_V1[:] = add_ones(Z0).T @ Dv1\n",
    "\n",
    "    Dv0 = Dv1 @ V1[1:, :].T * (1 - Z0**2)\n",
    "    grad_V0[:] = add_ones(X).T @ Dv0\n",
    "\n",
    "    return all_gradients\n",
    "\n",
    "# Apply network to data X\n",
    "def use(X):\n",
    "    X = (X - Xmeans) / Xstds\n",
    "    Z = np.tanh(add_ones(X) @ V0)\n",
    "    Z = np.tanh(add_ones(Z) @ V1)\n",
    "    Y = add_ones(Z) @ W\n",
    "    return Y * Tstds + Tmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain = X\n",
    "Ttrain = T\n",
    "Xval = X\n",
    "Tval = T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# Standardize data\n",
    "    \n",
    "Xmeans = Xtrain.mean(axis=0)\n",
    "Xstds = Xtrain.std(axis=0)\n",
    "Tmeans = Ttrain.mean(axis=0)\n",
    "Tstds = Ttrain.std(axis=0)\n",
    "\n",
    "XtrainS = (Xtrain - Xmeans) / Xstds\n",
    "TtrainS = (Ttrain - Tmeans) / Tstds\n",
    "XvalS = (Xval - Xmeans) / Xstds\n",
    "TvalS = (Tval - Tmeans) / Tstds\n",
    "# XtestS = (Xtest - Xmeans) / Xstds\n",
    "# TtestS = (Ttest - Tmeans) / Tstds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_weights[:] = np.arange(len(all_weights)) * 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.   , 0.001, 0.002, 0.003, 0.004, 0.005, 0.006, 0.007, 0.008,\n",
       "        0.009, 0.01 , 0.011, 0.012]),\n",
       " [array([[0.   , 0.001],\n",
       "         [0.002, 0.003]]),\n",
       "  array([[0.004, 0.005],\n",
       "         [0.006, 0.007],\n",
       "         [0.008, 0.009]]),\n",
       "  array([[0.01 ],\n",
       "         [0.011],\n",
       "         [0.012]])])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_weights, Ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.00268328 -0.00302491]\n",
      " [-0.00089443 -0.00034164]\n",
      " [ 0.00089443  0.00234164]\n",
      " [ 0.00268328  0.00502488]] [[0.00395968 0.00495395]\n",
      " [0.00399188 0.00499062]\n",
      " [0.00402408 0.00502729]\n",
      " [0.00405628 0.00506396]] [[0.010103  ]\n",
      " [0.0101038 ]\n",
      " [0.01010459]\n",
      " [0.01010539]]\n",
      "[[-0.00268328 -0.00302491]\n",
      " [-0.00089443 -0.00034164]\n",
      " [ 0.00089443  0.00234164]\n",
      " [ 0.00268328  0.00502488]] [[0.00395968 0.00495395]\n",
      " [0.00399188 0.00499062]\n",
      " [0.00402408 0.00502729]\n",
      " [0.00405628 0.00506396]] [[0.010103  ]\n",
      " [0.0101038 ]\n",
      " [0.01010459]\n",
      " [0.01010539]]\n",
      "SGD: Epoch 1 MSE=1.00010,1.00010\n"
     ]
    }
   ],
   "source": [
    "######################################################################\n",
    "# Ready to train!\n",
    "    \n",
    "optimizer = opt.Optimizers(all_weights)\n",
    "    \n",
    "method = 'sgd'\n",
    "\n",
    "if method == 'sgd':\n",
    "      \n",
    "    n_epochs = 1\n",
    "    learning_rate = 0.1\n",
    "    momentum = 0.\n",
    "    weight_penalty = 0\n",
    "    error_trace = optimizer.sgd(XtrainS, TtrainS, XvalS, TvalS, error_f, gradient_f,\n",
    "                                n_epochs=n_epochs, learning_rate=learning_rate,\n",
    "                                momentum=momentum, weight_penalty=weight_penalty,\n",
    "                                verbose=True)\n",
    "        \n",
    "elif method == 'adamw':\n",
    "\n",
    "    n_epochs = 10000\n",
    "    learning_rate = 0.005\n",
    "    weight_penalty = 0\n",
    "    error_trace = optimizer.adamw(XtrainS, TtrainS, XvalS, TvalS, error_f, gradient_f,\n",
    "                                  n_epochs=n_epochs, learning_rate=learning_rate,\n",
    "                                  weight_penalty=weight_penalty,\n",
    "                                  verbose=True)\n",
    "\n",
    "elif method == 'scg':\n",
    "\n",
    "    n_epochs = 2000\n",
    "    weight_penalty = 0.0\n",
    "    error_trace = optimizer.scg(XtrainS, TtrainS, XvalS, TvalS, error_f, gradient_f,\n",
    "                                n_epochs=n_epochs, \n",
    "                                weight_penalty=weight_penalty,\n",
    "                                verbose=True) \n",
    "    \n",
    "else:\n",
    "    print('method must be \\'sgd\\', \\'adamw\\' or \\'scg\\' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.00269309, -0.00303773],\n",
       "        [-0.0008978 , -0.00034605],\n",
       "        [ 0.00089749,  0.00234565],\n",
       "        [ 0.00269278,  0.0050373 ]]),\n",
       " array([[0.0039484 , 0.00494164],\n",
       "        [0.00398071, 0.00497843],\n",
       "        [0.00401302, 0.00501523],\n",
       "        [0.00404533, 0.00505203]]),\n",
       " array([[0.00909229],\n",
       "        [0.00909308],\n",
       "        [0.00909388],\n",
       "        [0.00909468]]))"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z0, Z1, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.001     ],\n",
       "       [0.001     , 0.00249999],\n",
       "       [0.002     , 0.00399998],\n",
       "       [0.00299999, 0.00549994]])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.tanh(add_ones(X) @ V0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-1.34164079],\n",
       "        [-0.4472136 ],\n",
       "        [ 0.4472136 ],\n",
       "        [ 1.34164079]]),\n",
       " array([[-0.00268328, -0.00302491],\n",
       "        [-0.00089443, -0.00034164],\n",
       "        [ 0.00089443,  0.00234164],\n",
       "        [ 0.00268328,  0.00502488]]),\n",
       " array([[0.00395968, 0.00495395],\n",
       "        [0.00399188, 0.00499062],\n",
       "        [0.00402408, 0.00502729],\n",
       "        [0.00405628, 0.00506396]]),\n",
       " array([[0.010103  ],\n",
       "        [0.0101038 ],\n",
       "        [0.01010459],\n",
       "        [0.01010539]])]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Zs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 1.51603124e-06,  1.98217653e-06],\n",
       "        [-7.19909102e-05, -9.40663907e-05]]),\n",
       " array([[ 1.11145896e-04,  1.21249680e-04],\n",
       "        [-1.05587719e-05, -1.15185542e-05],\n",
       "        [-1.57268813e-05, -1.71564392e-05]]),\n",
       " array([[1.01041953e-02],\n",
       "        [2.32194989e-05],\n",
       "        [3.09340617e-05]]))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_V0, grad_V1, grad_W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 1.51603124e-06,  1.98217653e-06],\n",
       "        [-7.19909102e-05, -9.40663907e-05]]),\n",
       " array([[ 1.11145896e-04,  1.21249680e-04],\n",
       "        [-1.05587719e-05, -1.15185542e-05],\n",
       "        [-1.57268813e-05, -1.71564392e-05]]),\n",
       " array([[1.01041953e-02],\n",
       "        [2.32194989e-05],\n",
       "        [3.09340617e-05]])]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-1.51603124e-07,  9.99801782e-04],\n",
       "        [ 2.00719909e-03,  3.00940664e-03]]),\n",
       " array([[0.00398889, 0.00498788],\n",
       "        [0.00600106, 0.00700115],\n",
       "        [0.00800157, 0.00900172]]),\n",
       " array([[0.00898958],\n",
       "        [0.01099768],\n",
       "        [0.01199691]]))"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V0, V1, W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00909256],\n",
       "       [0.00909335],\n",
       "       [0.00909415],\n",
       "       [0.00909494]])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_ones(Z1) @ W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ],\n",
       "       [-0.45973279],\n",
       "       [-0.45777798],\n",
       "       [ 0.64865886]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(error_trace)\n",
    "plt.axvline(optimizer.best_epoch, ymin=0, ymax=1, color='black', lw=4, alpha=0.2)\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(Xtrain, Ttrain, 'b.', label='Ttrain')\n",
    "plt.plot(Xtrain, use(Xtrain), 'b-', label='Ytrain')\n",
    "\n",
    "plt.plot(Xval, Tval, 'r.', label='Tval')\n",
    "plt.plot(Xtest, Ttest, 'c.', label='Ttest')\n",
    "plt.legend()\n",
    "\n",
    "title = f'{method}: RMSE Train {rmse_unstandardized(Ttrain, use(Xtrain), Tstds):.2f}'\n",
    "title += f' Val {rmse_unstandardized(Tval, use(Xval), Tstds):.2f}'\n",
    "title += f' Test {rmse_unstandardized(Ttest, use(Xtest), Tstds):.2f}'\n",
    "plt.title(title)\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
