{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6290112d",
   "metadata": {},
   "source": [
    "# Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a54f00c",
   "metadata": {},
   "source": [
    "modified from github repo linked to from https://peterbloem.nl/blog/transformers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2d79516",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as dist\n",
    "import time\n",
    "import random, math, sys, tqdm, math, gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31f29369",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# from util.py\n",
    "\n",
    "def d(tensor=None):\n",
    "    # return torch.device('cpu')\n",
    "    return torch.device('mps') if torch.backends.mps.is_available() else torch.device('cuda') if torch.cuda.is_available else torch.device('cpu')\n",
    "\n",
    "def slice_diag(matrix, l, dv=None):\n",
    "    \"\"\"\n",
    "    Take a batch of attention matrices for relative position encodings and slice out the relevant attentions. These\n",
    "    are the length l sequences starting at the diagonal\n",
    "    \"\"\"\n",
    "    if dv is None:\n",
    "        dv = d(matrix)\n",
    "\n",
    "    h, w = matrix.size(-2), matrix.size(-1)\n",
    "    assert w == 2 * l -1, f'(h, w)= {(h, w)}, l={l}'\n",
    "    rest = matrix.size()[:-2]\n",
    "\n",
    "    matrix = matrix.view(-1, h, w)\n",
    "    b, h, w = matrix.size()\n",
    "\n",
    "    result = matrix.view(b, -1)\n",
    "    result = torch.cat([result, torch.zeros(b, l, device=dv)], dim=1)\n",
    "    assert result.size() == (b, 2 * l * l), f'result.size() {result.size()}'\n",
    "\n",
    "    result = result.view(b, l, 2*l)\n",
    "    result = result[:, :, :l]\n",
    "\n",
    "    result = result.view(*rest, h, l)\n",
    "    return result\n",
    "\n",
    "def mask_(matrices, maskval=0.0, mask_diagonal=True):\n",
    "    \"\"\"\n",
    "    Masks out all values in the given batch of matrices where i <= j holds,\n",
    "    i < j if mask_diagonal is false\n",
    "    \"\"\"\n",
    "\n",
    "    h, w = matrices.size(-2), matrices.size(-1)\n",
    "\n",
    "    indices = torch.triu_indices(h, w, offset=0 if mask_diagonal else 1)\n",
    "    matrices[..., indices[0], indices[1]] = maskval\n",
    "\n",
    "def sample_batch(data, length, batch_size):\n",
    "    \"\"\"\n",
    "    Takes the data (a single sequence of tokens) and slices out a batch of subsequences to provide as input to the model.\n",
    "\n",
    "    For each input instance, it also slices out the sequence that is shifted one position to the right, to provide as a\n",
    "    target for the model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Sample the starting indices of the sequences to slice out.\n",
    "    starts = torch.randint(size=(batch_size,), low=0, high=data.size(0) - length - 1)\n",
    "\n",
    "    # Slice out the input sequences\n",
    "    seqs_inputs  = [data[start:start + length] for start in starts]\n",
    "    # -- the start index is the one we just sampled, and the end is exactly 'lentgh' positions after that.\n",
    "    seqs_target = [data[start + 1:start + length + 1] for start in starts]\n",
    "    # -- The target is the same sequence as input, except one character ahead (we are asking the model to predict the\n",
    "    #    next character at each position)\n",
    "\n",
    "    # We now have two lists of torch vectors, which we can concatenate into matrices of batch_size-by-length\n",
    "    inputs = torch.cat([s[None, :] for s in seqs_inputs], dim=0).to(torch.long)\n",
    "    target = torch.cat([s[None, :] for s in seqs_target], dim=0).to(torch.long)\n",
    "    # -- Note that we add a singleton dimenson to each vector, s[None.,:], and then concatenate along that dimension.\n",
    "\n",
    "    return inputs, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22c4435b",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# modules.py\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, emb, heads=8, mask=False, kqnorm=False):\n",
    "        super().__init__()\n",
    "\n",
    "        assert emb % heads == 0, f'Embedding dimension ({emb}) should be divisible by nr. of heads ({heads})'\n",
    "\n",
    "        self.emb = emb\n",
    "        self.heads = heads\n",
    "        self.mask = mask\n",
    "\n",
    "        s = emb // heads\n",
    "        # - We will break the embedding into `heads` chunks and feed each to a different attention head\n",
    "\n",
    "        self.tokeys    = nn.Linear(emb, emb, bias=False)\n",
    "        self.toqueries = nn.Linear(emb, emb, bias=False)\n",
    "        self.tovalues  = nn.Linear(emb, emb, bias=False)\n",
    "\n",
    "        self.unifyheads = nn.Linear(emb, emb)\n",
    "\n",
    "        self.kqnorm = kqnorm\n",
    "        if kqnorm:\n",
    "            self.kln = nn.LayerNorm([s])\n",
    "            self.qln = nn.LayerNorm([s])\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        b, t, e = x.size()\n",
    "        h = self.heads\n",
    "        assert e == self.emb, f'Input embedding dim ({e}) should match layer embedding dim ({self.emb})'\n",
    "\n",
    "        s = e // h\n",
    "\n",
    "        keys    = self.tokeys(x)\n",
    "        queries = self.toqueries(x)\n",
    "        values  = self.tovalues(x)\n",
    "\n",
    "        keys    = keys.view(b, t, h, s)\n",
    "        queries = queries.view(b, t, h, s)\n",
    "        values  = values.view(b, t, h, s)\n",
    "\n",
    "        if self.kqnorm:\n",
    "            keys = self.kln(keys)\n",
    "            queries = self.qln(queries)\n",
    "\n",
    "        # -- We first compute the k/q/v's on the whole embedding vectors, and then split into the different heads.\n",
    "        #    See the following video for an explanation: https://youtu.be/KmAISyVvE1Y\n",
    "\n",
    "        # Compute scaled dot-product self-attention\n",
    "\n",
    "        # - fold heads into the batch dimension\n",
    "        keys = keys.transpose(1, 2).contiguous().view(b * h, t, s)\n",
    "        queries = queries.transpose(1, 2).contiguous().view(b * h, t, s)\n",
    "        values = values.transpose(1, 2).contiguous().view(b * h, t, s)\n",
    "\n",
    "        queries = queries / (e ** (1/4))\n",
    "        keys    = keys / (e ** (1/4))\n",
    "        # - Instead of dividing the dot products by sqrt(e), we scale the keys and values.\n",
    "        #   This should be more memory efficient\n",
    "\n",
    "        # - get dot product of queries and keys, and scale\n",
    "        dot = torch.bmm(queries, keys.transpose(1, 2))\n",
    "\n",
    "        assert dot.size() == (b*h, t, t)\n",
    "\n",
    "        if self.mask: # mask out the upper half of the dot matrix, excluding the diagonal\n",
    "            mask_(dot, maskval=float('-inf'), mask_diagonal=False)\n",
    "\n",
    "        dot = F.softmax(dot, dim=2)\n",
    "        # - dot now has row-wise self-attention probabilities\n",
    "\n",
    "        # apply the self attention to the values\n",
    "        out = torch.bmm(dot, values).view(b, h, t, s)\n",
    "\n",
    "        # swap h, t back, unify heads\n",
    "        out = out.transpose(1, 2).contiguous().view(b, t, s * h)\n",
    "\n",
    "        return self.unifyheads(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6653b424",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, emb, heads, mask, seq_length, ff_hidden_mult=4, \n",
    "                 dropout=0.0, attention_type='default',\n",
    "                 pos_embedding=None, sa_kwargs={}):\n",
    "        super().__init__()\n",
    "\n",
    "        if attention_type == 'default':\n",
    "            self.attention = SelfAttention(emb, heads=heads, mask=mask, **sa_kwargs)\n",
    "        elif attention_type == 'alt':\n",
    "            self.attention = SelfAttentionAlt(emb, heads=heads, mask=mask)\n",
    "        elif attention_type == 'wide':\n",
    "            self.attention = SelfAttentionWide(emb, heads=heads, mask=mask)\n",
    "        elif attention_type == 'gpt2':\n",
    "            self.attention = SelfAttentionGPT2(emb, heads=heads, mask=mask)\n",
    "        elif attention_type == 'narrow':\n",
    "            self.attention = SelfAttentionNarrow(emb, heads=heads, mask=mask)\n",
    "        elif attention_type == 'relative':\n",
    "            assert pos_embedding is not None\n",
    "            self.attention = SelfAttentionRelative(emb, heads=heads, mask=mask, pos_embedding=pos_embedding)\n",
    "        else:\n",
    "            raise Exception(f'Self-attention type {type} not recognized.')\n",
    "            \n",
    "        self.mask = mask\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(emb)\n",
    "        self.norm2 = nn.LayerNorm(emb)\n",
    "\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(emb, ff_hidden_mult * emb),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_hidden_mult * emb, emb)\n",
    "        )\n",
    "\n",
    "        self.do = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        attended = self.attention(x)\n",
    "        x = self.norm1(attended + x)\n",
    "        x = self.do(x)\n",
    "        fedforward = self.ff(x)\n",
    "        x = self.norm2(fedforward + x)\n",
    "        x = self.do(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "######################################################################\n",
    "# transformers.py\n",
    "\n",
    "class GTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer for generating text (character by character).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, emb, heads, depth, seq_length, num_tokens, attention_type='default'):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_tokens = num_tokens\n",
    "        self.token_embedding = nn.Embedding(embedding_dim=emb, num_embeddings=num_tokens)\n",
    "        self.pos_embedding = nn.Embedding(embedding_dim=emb, num_embeddings=(seq_length * 2 - 1 if attention_type=='relative' else seq_length))\n",
    "\n",
    "        tblocks = []\n",
    "        for i in range(depth):\n",
    "            tblocks.append(TransformerBlock(emb=emb, heads=heads, seq_length=seq_length, mask=True, \n",
    "                                            attention_type=attention_type, pos_embedding=self.pos_embedding))\n",
    "\n",
    "        self.tblocks = nn.Sequential(*tblocks)\n",
    "        self.toprobs = nn.Linear(emb, num_tokens)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        tokens = self.token_embedding(x)\n",
    "        b, t, e = tokens.size()\n",
    "\n",
    "        positions = self.pos_embedding(torch.arange(t, device=d()))[None, :, :].expand(b, t, e)\n",
    "        x = tokens + positions\n",
    "\n",
    "        x = self.tblocks(x)\n",
    "        x = self.toprobs(x.view(b*t, e)).view(b, t, self.num_tokens)\n",
    "\n",
    "        return F.log_softmax(x, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb08c366",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# generate.py\n",
    "\n",
    "# NB, the enwik8 data contains tokens from 9 to 240, but we'll round up to the nearest\n",
    "# power of two.\n",
    "NUM_TOKENS = 256\n",
    "\n",
    "def enwik8(path, n_train=int(90e6), n_valid=int(5e6), n_test=int(5e6)):\n",
    "    \"\"\"\n",
    "    Load the enwik8 dataset from the Hutter challenge.\n",
    "\n",
    "    Adapted from https://github.com/openai/blocksparse/blob/master/examples/transformer/enwik8.py\n",
    "    :param path:\n",
    "    :param n_train:\n",
    "    :param n_valid:\n",
    "    :param n_test:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    with gzip.open(path) if path.endswith('.gz') else open(path) as file:\n",
    "        # X = np.fromstring(file.read(n_train + n_valid + n_test), dtype=np.uint8)\n",
    "        X = np.frombuffer(file.read(n_train + n_valid + n_test), dtype=np.uint8)\n",
    "        trX, vaX, teX = np.split(X, [n_train, n_train + n_valid])\n",
    "        return torch.from_numpy(trX), torch.from_numpy(vaX), torch.from_numpy(teX)\n",
    "\n",
    "def sample(lnprobs, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Sample an element from a categorical distribution\n",
    "    :param lnprobs: Outcome log-probabilities\n",
    "    :param temperature: Sampling temperature. 1.0 follows the given distribution,\n",
    "        0.0 returns the maximum probability element.\n",
    "    :return: The index of the sampled element.\n",
    "    \"\"\"\n",
    "\n",
    "    if temperature == 0.0:\n",
    "        return lnprobs.argmax()\n",
    "\n",
    "    p = F.softmax(lnprobs / temperature, dim=0)\n",
    "    cd = dist.Categorical(p)\n",
    "\n",
    "    return cd.sample()\n",
    "\n",
    "def sample_sequence(model, seed, max_context, length=600, temperature=0.5, verbose=False):\n",
    "    \"\"\"\n",
    "    Sequentially samples a sequence from the model, token by token.\n",
    "\n",
    "    :param model:\n",
    "    :param seed: The sequence to start with.\n",
    "    :param length: The total number of characters to sample.\n",
    "    :param temperature: The sampling temperature.\n",
    "    :param verbose: If true, the sampled sequence is also printed as it is sampled.\n",
    "\n",
    "    :return: The sampled sequence, including the seed.\n",
    "    \"\"\"\n",
    "\n",
    "    sequence = seed.detach().clone()\n",
    "\n",
    "    if verbose: # Print the seed, surrounded by square brackets\n",
    "        print('\\nGiven this sequence-------\\n')  # , end='', flush=True)\n",
    "        for c in seed:\n",
    "            print(str(chr(c)), end='', flush=True)\n",
    "        print('\\n\\n-------the transformer continued with\\n') #, end='', flush=True)\n",
    "\n",
    "    for _ in range(length):\n",
    "\n",
    "        # Input is the tail end of the sampled sequence (as many tokens as the model can handle)\n",
    "        input = sequence[-max_context:]\n",
    "\n",
    "        # Run the current input through the model\n",
    "        output = model(input[None, :])\n",
    "\n",
    "        # Sample the next token from the probabilitys at the last position of the output.\n",
    "        c = sample(output[0, -1, :], temperature)\n",
    "\n",
    "        if verbose:\n",
    "            print(str(chr(max(32, c))), end='', flush=True)\n",
    "\n",
    "        sequence = torch.cat([sequence, c[None]], dim=0) # Append the sampled token to the sequence\n",
    "\n",
    "    print()\n",
    "    return seed\n",
    "\n",
    "def create_train_test(arg):\n",
    "    \n",
    "    data_train, data_val, data_test = enwik8('enwik8.gz')\n",
    "    data_train, data_test = (torch.cat([data_train, data_val], dim=0), data_test) \\\n",
    "                            if arg.final else (data_train, data_val)\n",
    "\n",
    "    if False:\n",
    "        orig_len = len(data_train)\n",
    "        n = int(orig_len * 0.1)\n",
    "        data_train = data_train[:n]\n",
    "        print(f'Reducing size of data_train from {orig_len} to {n} ({n / orig_len})')\n",
    "        \n",
    "    # create the model\n",
    "    model = GTransformer(emb=arg.embedding_size, heads=arg.num_heads,\n",
    "                         depth=arg.depth, seq_length=arg.context, num_tokens=NUM_TOKENS,\n",
    "                         attention_type=arg.attention_type)\n",
    "    if torch.cuda.is_available():\n",
    "        model.cuda()\n",
    "    elif torch.backends.mps.is_available():\n",
    "        model.to('mps')\n",
    "\n",
    "    opt = torch.optim.Adam(lr=arg.lr, params=model.parameters())\n",
    "\n",
    "    # Linear learning rate warmup\n",
    "    sch = torch.optim.lr_scheduler.LambdaLR(opt, lambda i: min(i / (arg.lr_warmup / arg.batch_size), 1.0))\n",
    "\n",
    "    # Training loop\n",
    "    # -- We don't loop over the data, instead we sample a batch of random subsequences each time. This is not strictly\n",
    "    #    better or worse as a training method, it's just a little simpler.\n",
    "    #\n",
    "    instances_seen = 0\n",
    "    for i in tqdm.trange(arg.num_batches):\n",
    "\n",
    "        opt.zero_grad()\n",
    "\n",
    "        source, target = sample_batch(data_train, length=arg.context, batch_size=arg.batch_size)\n",
    "        instances_seen += source.size(0)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            source, target = source.cuda(), target.cuda()\n",
    "        elif torch.backends.mps.is_available():\n",
    "            source, target = source.to('mps'), target.to('mps')\n",
    "\n",
    "        output = model(source) # forward pass\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = F.nll_loss(output.transpose(2, 1), target, reduction='mean')\n",
    "\n",
    "        # tbw.add_scalar('transformer/train-loss', float(loss.item()) * util.LOG2E, i * arg.batch_size, instances_seen)\n",
    "        # tbw.add_scalar('transformer/time-forward', t, instances_seen)\n",
    "\n",
    "        loss.backward() # backward pass\n",
    "\n",
    "        # clip gradients\n",
    "        # -- If the total gradient vector has a length > x, we clip it back down to x.\n",
    "        if arg.gradient_clipping > 0.0:\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), arg.gradient_clipping)\n",
    "\n",
    "        opt.step() # stochastic gradient descent step\n",
    "        sch.step() # update the learning rate\n",
    "\n",
    "        # Validate every `arg.test_every` steps. First we compute the\n",
    "        # compression on the validation data (or a subset),\n",
    "        # then we generate some random text to monitor progress.\n",
    "        if i != 0 and (i % arg.test_every == 0 or i == arg.num_batches - 1):\n",
    "            with torch.no_grad():\n",
    "\n",
    "                ## Sample and print a random sequence\n",
    "\n",
    "                # Slice a random seed from the test data, and sample a continuation from the model.\n",
    "                seedfr = random.randint(0, data_test.size(0) - arg.context)\n",
    "                seed = data_test[seedfr:seedfr + arg.context].to(torch.long)\n",
    "\n",
    "                if torch.cuda.is_available():\n",
    "                    seed = seed.cuda()\n",
    "\n",
    "                sample_sequence(model, seed=seed, max_context=arg.context, verbose=True, length=arg.sample_length)\n",
    "\n",
    "                ## Compute validation bits per byte\n",
    "\n",
    "                upto = data_test.size(0) if i == arg.num_batches - 1 else arg.test_subset\n",
    "                data_sub = data_test[:upto]\n",
    "\n",
    "                # bits_per_byte = util.compute_compression(model, data_sub, context=arg.context, batch_size=arg.test_batchsize)\n",
    "                # -- Since we're not computing gradients, we can increase the batch size a little from what we used in\n",
    "                #    training.\n",
    "\n",
    "                # print(f'epoch{i}: {bits_per_byte:.4} bits per byte')\n",
    "                # tbw.add_scalar(f'transformer/eval-loss', bits_per_byte, i * arg.batch_size, instances_seen)\n",
    "                # -- 0.9 bit per byte is around the state of the art."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a6779cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|███▌                                                                                     | 2000/50000 [03:24<1:22:29,  9.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Given this sequence-------\n",
      "\n",
      "ovisional Government.  Furthermore, it instructed all IRA units to reaffirm their allegiance to the Irish Republic on [[2 April]].\n",
      "\n",
      "On [[9 April]], the (anti-Treaty) Army Convention reconvened in Dublin. It adopted a new constitution and elected a new 16-m\n",
      "\n",
      "-------the transformer continued with\n",
      "\n",
      "ast bof the win sthe in ce tofing the ton thow [[huste e tof coromen a end pacater o and and the of cospermers of ingncerents te tof the the come o the the wit the the ple the a sthe of ate the the cowe the leallll be the the wand the as be ame gee e the the the the and the a the the ain at the bat win the the sean the an the the te a we inoo t the of isthe the the gne are t the the the the the bume tir of the the ofus the the the the be weve trore so the the se the ane te af tre wene see tren te the tof the t thein the it the the the ame the tor the the ase fre se tole the te the the the the "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|███▌                                                                                    | 2001/50000 [03:30<26:05:47,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|███████                                                                                  | 4000/50000 [06:55<1:20:09,  9.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Given this sequence-------\n",
      "\n",
      "-like dome.\n",
      "*HAL was also &quot;featured&quot; in a short commercial by [[Apple Computer]] in the year [[1999]] in which he asks Dave if he likes his Mac more than him (because Macs would not undergo the same potential problems with the &quot;[[Year 2000 p\n",
      "\n",
      "-------the transformer continued with\n",
      "\n",
      "rined to contric the streation he the singne a or of the and inted of a comp the contromed to the [[Iman of the commamed, and on the ase on a sell for of clanded of the [[comporition]].  The as the had the of the wo the and to he prodent of the of the spert of so in that of ithe many of the [[artugation]].  The and a a parter of the reather inter the was he and of the a calllequor was in the be and to the sprow.  on the stat contions to a comp onit the the for abot was and as prond of a the ore a const on of a the ongent complicity (shasord of the the ''sperined of the the chirsture of the a c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|███████                                                                                 | 4001/50000 [07:01<25:57:19,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|██████████▋                                                                              | 6000/50000 [10:27<1:14:29,  9.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Given this sequence-------\n",
      "\n",
      "sion of &quot;henotheism&quot;, from ''kath heno theon'' &amp;ndash; &quot;one god at a time&quot;. Henotheism is similar but less exclusive than [[monolatry]] because a monolator worships only one god, while the henotheist may worship any within the [[pan\n",
      "\n",
      "-------the transformer continued with\n",
      "\n",
      "t anding as in the same of the movers]]. It also the the ching of exing a for many was contitial the the consition in meal stater be the const and book as the reament of the game and the sechnology and the form the mole asstems and the state for of the contral of the can of incent the artage.  The many the the company and the ether of the from of the sear awar of the many in and set bett and the [[controlatic of regan]] the the the was the a proced the some internes of the can of the lay and arther the pland show the arganst in the and the process and the can since. The not the [[chulle]], whi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|██████████▌                                                                             | 6001/50000 [10:33<24:40:18,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|██████████████▏                                                                          | 8000/50000 [13:58<1:12:38,  9.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Given this sequence-------\n",
      "\n",
      "or other uses, including people named &quot;Islam&quot;, see [[Islam (disambiguation)]]}}\n",
      "{{Islam}}\n",
      "{{portal}}\n",
      "'''Islam''' ([[Arabic language|Arabic]]: {{Ar|Ø§ÙØ¥Ø³ÙØ§Ù",
      "}}; ''{{ArabDIN|al-islÄm}}'' ({{Audio|ar-al_islam.ogg|listen}}), &quot;submission (t\n",
      "\n",
      "-------the transformer continued with\n",
      "\n",
      "he stylet)&quot; (k. [[Alange|Then Bargand]]) &quot; ([[The Sockera]])&quot; (Borg), and the [[Lasegen (congly)|elight]] ([[Norand Standon)|Baberthon]]), [[Sayperands (sech)|Althong]] and [[Pesial (many)|Franct]] ([[1999]]) and the [[Many Liston]]). The arteanct are squing the socisions the [[United Danadine|Song States]] and [[United Sparting Action Presentice]] ([[1988]]) ''[[Rebert Mathet Party]]'' and [[1974]] to [[1970]] and [[New 1900]]. The tame stim in [[1994]]. The [[1975]] the [[begine Minish Kernge]] are [[1960]], [[1994]], [[1980]], [[1998]], [[151]] [[1996]], [[1992]], [[Arance|Ge"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|██████████████                                                                          | 8001/50000 [14:05<23:55:40,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█████████████████▌                                                                      | 10000/50000 [17:30<1:07:55,  9.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Given this sequence-------\n",
      "\n",
      "Î¬Î¸Î¿Ï, pÃ¡thos'' (suffering), is a [http://www.jr2.ox.ac.uk/bandolier/booth/alternat/AT014.html controversial] system of [[alternative medicine]]. The model of homeopathy was developed by the [[Saxony|Saxon]] physician [[Samuel Hahnemann]] ([[1755]]&am\n",
      "\n",
      "-------the transformer continued with\n",
      "\n",
      "p;#[[1988]]) and [[Conter Swork]]) and the [[She Comper Christian Contrice]] and the [[Dish Book]] and [[Minishader]], [[Dary of Coliticale]] becamed the [[Tingory Prince (rome and States]]) [[Category:Sea Persian Sanda Bark Centrian Manutine Antiment Pressidenting New Monsantional Chinolding States of the Scholetic and Resers and Developed Meltish Analian Dates of the Many Persident and C. The Baserian are of the Brite Riser (Resian Corber), and the Bish Aritined Statines Contral Karin President in the Shold Center Strees Compire of the Seath Peritions of the [[Rolegican Derice Compen]] and t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█████████████████▍                                                                     | 10001/50000 [17:37<22:40:36,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|█████████████████████                                                                   | 12000/50000 [21:01<1:05:37,  9.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Given this sequence-------\n",
      "\n",
      "e opinion, and 10 the highest.  Each voter determines his own standards upon which to participate, and to rate each film.  The received votes are weighted according to the demographic distribution in order to compensate for the fact that IMDb users are not\n",
      "\n",
      "-------the transformer continued with\n",
      "\n",
      " of the top of the standard bird the toles of the long of the state consider to season and and one of comproses.  This depiode to the an other spection of the city of the proposing the believes and the other form band an a comput the opposing to all be comparation of the are only in region to a conne to a light of exigns, it call be or other country and the interrage makes to the was one all with nort to be a later of describete of the callege compaint important that a was a calles in the startles of the experial concept in the island to is can the comple to the [[Seron of the Comperor]].  ==="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|████████████████████▉                                                                  | 12001/50000 [21:08<21:33:11,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|████████████████████████▋                                                               | 14000/50000 [24:32<1:02:33,  9.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Given this sequence-------\n",
      "\n",
      "bours since the 1963-1965  [[Indonesia-Malaysia confrontation|confrontation]] with [[Malaysia]], although competing [[South China Sea]] claims, where Indonesia has large natural gas reserves, concern the Indonesian government. Without a credible external t\n",
      "\n",
      "-------the transformer continued with\n",
      "\n",
      "o concent to the consistic capable to [[New Deammany]] and the [[Central Recorde]] and they are and [[time of the Congress]] and the [[United States of the Conzing of Congress]]. The original and a control on the cannoning in the [[United States]] in [[Kamme]], and [[Constand]] in the [[United States]] and [[Minres]]. He the head no see in the [[Damen South]] in [[George Press]] the [[Same Games]]. To the tend of the [[Massachine work]] and the [[Masson Coland]] and [[Permont of Chammon|the Greess]] of the term was family in the [[Cast of the Communical of the Central Calenday]] the company of"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|████████████████████████▎                                                              | 14001/50000 [24:39<19:59:16,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|████████████████████████████▊                                                             | 16000/50000 [28:03<58:19,  9.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Given this sequence-------\n",
      "\n",
      " early 1990s\n",
      "*[[Hardcore punk]], a more intense version of punk rock\n",
      "**[[Hardcore Emo]], a style of music that existed primarily in the early-mid 90s, also known as Chaotic Emo\n",
      "**[[Christian hardcore]], a form of hardcore punk music featuring evangelical C\n",
      "\n",
      "-------the transformer continued with\n",
      "\n",
      "hristian and missions are of the first of the first in the [[Christian Christianity|Christian characterist depected]].  The [[Christian Christianity]] and [[Christian Christian University]] and [[Christian Chistian Christians]], and an even by [[Christian Elistrands|Chistian Christians]] and [[Christian Christians]] and [[Christian Christian of the Christians|Christianian Christians]]. The [[Christian Christians]] and the [[Christian Chistian Chistian Churchistians|Christianity]] and [[Christian of Scients Christians of Christian Changell Christians]].  ==See also== *[[Christian Christians Chi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███████████████████████████▊                                                           | 16001/50000 [28:10<18:44:51,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|████████████████████████████████▍                                                         | 18000/50000 [31:34<53:27,  9.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Given this sequence-------\n",
      "\n",
      "rtugal]], [[Philippines]], [[Bulgaria]], [[Nicaragua]] and [[Italy]] left or are planning to leave as well. Other nations (such as [[Australia]], [[Denmark]] and [[Poland]]) continue commitment in Iraq.\n",
      "\n",
      "On [[28 June]] [[2004]], the occupation was formally\n",
      "\n",
      "-------the transformer continued with\n",
      "\n",
      " served the [[English English]] and [[Austrial Republic]] and [[Michal]] and [[Aeral Standar]]. The [[United II]] [[Frand Standard]], [[Australia]] and [[Anglicary]], [[United Standard]] [[Contralis]] and [[Juan Sandard]] [[Sandard Surun]] and [[Natish Country]], [[Randia]] and [[Sancon]], [[Tangdon]] and [[Handlack]] and [[Angland]], [[United Standard]] and [[Standard]], [[Animala]], [[Natary Andrese United Standards]].  ===Standardars=== [[Image:Andrand Andard film ended by [[Sandard Anderard]] and [[Andard Standard]], [[Sandard Andreald]], [[Andron Sandard]], [[Natheral Andreald]], [[Andred"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███████████████████████████████▎                                                       | 18001/50000 [31:40<16:19:07,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████████████████████████████████████                                                      | 20000/50000 [35:04<50:19,  9.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Given this sequence-------\n",
      "\n",
      "icism is based on theoretical understanding of physical/chemical/biological properties of matter and its interaction in the biological sphere. Regardless of whether the dilution medium is water or alcohol, the existing theory of [[molecular physics]] does \n",
      "\n",
      "-------the transformer continued with\n",
      "\n",
      "the being of the players, but the voice of the [[political publical]] is the point for the country and also and country.  He was the country of the point of the set of example not the country of the country of the early of the world be countrally and country.  The country of the was also also the present of the country discussed because the name of the substant the second of the [[Fancist of the Bankada]], which he was the the [[United States of the Games of the [[Country of Mages]].  The was an a real of the [[Natural]] and [[United States|Sates Sala]] and [[Satural of Act]] and between the ["
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|██████████████████████████████████▊                                                    | 20001/50000 [35:11<16:05:07,  1.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|███████████████████████████████████████▌                                                  | 22000/50000 [38:35<47:02,  9.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Given this sequence-------\n",
      "\n",
      "0 RBI.  He finished .007 percentage points behind [[Tommy Davis]] in batting.  Aaron became the 3rd member of the [[30-30 club]] behind [[Ken Williams (baseball player)|Ken Williams]] and [[Willie Mays]].  He again finished 3rd in the MVP voting.\n",
      "\n",
      "On [[Sep\n",
      "\n",
      "-------the transformer continued with\n",
      "\n",
      "tience Services]] and [[New York | years of the English Paron]] clear [[Regother of Heather (1994)|Sea]] [[New York (1706)]] to [[French New York]] [[Shoeld Colomy]] and [[Second Color]].  The [[Second the Inding the Shoold of Become of the New Zeald of United Society Color of Colomy] from the [[Slame Shood Colomer of Colomeration]] and which we include would be the [[Shood of Colomeration]] simply and discovered by [[Romeration of Colomeration]]. The [[South Colomeration Colomeration]] colour of the colomeration of the [[United States]] and [[Colomeration Colomeration]] and [[Colomeration of "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|██████████████████████████████████████▎                                                | 22001/50000 [38:41<15:10:47,  1.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|███████████████████████████████████████████▏                                              | 24000/50000 [42:04<44:36,  9.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Given this sequence-------\n",
      "\n",
      "rocessing and atmospheric nuclear weapons tests have now swamped the natural signal for this isotope. I-129 was used in rainwater studies following the [[Chernobyl accident]]. It also has been used as a ground-water tracer and as an indicator of nuclear wa\n",
      "\n",
      "-------the transformer continued with\n",
      "\n",
      "s trained to be some than its in the president of the aircles.  A main the same crisiden was a climber of the [[Chernology]] to an early [[Chernology]] and the consider.  The nuclear consider was not be a considerable considered to the all chernology chernology and a considerable for the earlier Asia and considers are produced for the opput of the revolution of the [[Silveral Silvers]] were are such as [[Alexander Produces]]. The residence of the [[Alexander Alexander]] in [[Silvers Silvers]], and the term of the [[Tony Tony]] can be leaders.  The [[Silvers]] are offered the silvers of [[Silve"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|█████████████████████████████████████████▊                                             | 24001/50000 [42:11<14:07:06,  1.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|██████████████████████████████████████████████▊                                           | 26000/50000 [45:34<40:41,  9.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Given this sequence-------\n",
      "\n",
      "evision>\n",
      "  </page>\n",
      "  <page>\n",
      "    <title>Hammurabi</title>\n",
      "    <id>14358</id>\n",
      "    <revision>\n",
      "      <id>41629852</id>\n",
      "      <timestamp>2006-02-28T17:20:06Z</timestamp>\n",
      "      <contributor>\n",
      "        <username>Kmf164</username>\n",
      "        <id>94080</id>\n",
      "      </cont\n",
      "\n",
      "-------the transformer continued with\n",
      "\n",
      "ributor>       <comment>/* According</comment>       <text xml:space=\"preserve\">#REDIRECT [[History of Gates]] ([[War Farme]]), [[Gates (the Gates of Gates]] ([[Gates of Gates]]) and [[Gates (the Gates of the Gates)|Gates]] ([[Micrature of Gates]]) ([[Gates of Gates]]) ([[Gates of Gates]] ([[Gates of Hones]]) and [[Gates of new]] of the [[Gates of Gates]] ([[Gates Micrates]] ([[Opera]]) and [[Gates]] [[Gates of Gates]]. Hones on the world of the [[India]] [[Grates of the Gates]] and [[Nigary of Gates]] ([[Gates of the Paris]] of the [[Paris of the Gates]]), and [[Gates of Bates]] ([[Wateship o"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████████████████████████████████████████████▏                                         | 26001/50000 [45:40<12:51:39,  1.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|██████████████████████████████████████████████████▍                                       | 28000/50000 [49:04<37:46,  9.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Given this sequence-------\n",
      "\n",
      "ice, Israeli men become part of the IDF reserve forces, and are usually required to serve several weeks every year as reservists until their 40s.\n",
      "\n",
      "While the [[International Atomic Energy Agency]] suspects Israel to be a state possessing nuclear weapons, th\n",
      "\n",
      "-------the transformer continued with\n",
      "\n",
      "e contrasing the state of the serveral comparation of the Comparation State of the [[1911]], who was personal to have of the [[Comparation Comparation Comparation]].  He was often the [[Comparation Comparation]] of the Comparation of the [[Comparation Party of Comparation of Comparation]] (in its often very [[Comparation]] and [[Comparation Comparation Comparation of Comparation of Comparation of Comparation Comparation of Uril Comparation Comparation in Comparation Comparation Comparation Comparation of Comparation Comparation Comparations and Comparation Comparation Comparations and Comparat"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|████████████████████████████████████████████████▋                                      | 28001/50000 [49:10<11:39:25,  1.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████████████████████████████████████████████████████                                    | 30000/50000 [52:34<33:29,  9.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Given this sequence-------\n",
      "\n",
      "lt;sub&gt;0&lt;/sub&gt; if it is holomorphic on some neighborhood of ''z''&lt;sub&gt;0&lt;/sub&gt;. We say that ''f'' is holomorphic on some non-open set ''A'' if it is holomorphic in an open set containing ''A''.\n",
      "\n",
      "An equivalent definition is the following\n",
      "\n",
      "-------the transformer continued with\n",
      "\n",
      " the every of [[Air Aircraft]] in the [[Columbia]] in [[Aircraft]] in [[1999]] and [[Aircraft Aircraft]]. In [[1993]], the [[Michigarian Aircraft]] in the [[Second Aircraft]] and [[Second Aircraft]], [[Michine Aircraft]], [[Latin Airchaer Aircraft]], [[Second Airchaer]], [[Macigraft Aircraft]], [[Macinto]], [[Macinton]], [[Secondence]], [[Second Aircraft]], [[Macinton]], [[Second Aircraft]], [[Condencerto]], [[John Aircraft of Aircraft]], [[Society of Aircraft]], [[Saint Aircraft]], [[Condencertor]], [[Condencertor]], [[The Condencertor]], [[Condencertor]], [[Condencertory]], [[Citricium]], [["
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|████████████████████████████████████████████████████▏                                  | 30001/50000 [52:40<10:40:51,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|█████████████████████████████████████████████████████████▌                                | 32000/50000 [56:04<30:22,  9.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Given this sequence-------\n",
      "\n",
      "ttp://www.iowadnr.com/fish/fishing/lakes/ode58.html], Saylorville Lake, Lake Red Rock, and Rathbun Lake. \n",
      "\n",
      "The topography of the state is gently rolling [[plains]]. [[Loess]] hills lie along the western border of the state. Some of these are several hundre\n",
      "\n",
      "-------the transformer continued with\n",
      "\n",
      "d or all of the animal topography statements to resident show observers and the name of the [[Shinda Shindands]] consider and some was a mental of the most US and the order of the book of the [[Shindands]]. In [[1860]] after the word of the [[Shindands]], and was assigned by [[Messach]]. In the [[Franks]] is also the [[Image:Roman Temphan American and the United States and the American Republic emphasing and the [[Presidence of Australia]] in [[Shindands]] and the [[United States of the Septain]].  ===Bournaries=== ''See also: [[Bausian American secondaries]]'' *[[Bause Secondary Instituted Se"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|████████████████████████████████████████████████████████▎                               | 32001/50000 [56:10<9:39:13,  1.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|█████████████████████████████████████████████████████████████▏                            | 34000/50000 [59:33<27:05,  9.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Given this sequence-------\n",
      "\n",
      "ts of Iqaluit are called ''Iqalummiut'' (singular: ''Iqalummiuq''). \n",
      "\n",
      "Iqaluit has the distinction of being the smallest Canadian capital city in terms of population and the only capital that cannot be accessed from the rest of Canada via a [[highway]].\n",
      "\n",
      "Ab\n",
      "\n",
      "-------the transformer continued with\n",
      "\n",
      "lueaution terms in the [[Germany]] population of the [[European Council]] and [[archaeology]], and individual terms and their interests are threated.  == See also == [[Image:Starting.png|thumb|right|200px|The [[West West West West West West West Brown West Island West West March of the U.S. Chospall Starting West Brown Brown Brown The Starting Lord Brown Brown Hospital Brown Starting And Michael Albert Starting Starting Original Red Starting Starting Court. In the beached the Red Albert Press, and the artist of the English Ander Press. In the Starting New The Albert Starting Starting the Terri"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|███████████████████████████████████████████████████████████▊                            | 34001/50000 [59:39<8:34:10,  1.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████████████████████████████████████████████████████████████▎                        | 36000/50000 [1:03:03<23:38,  9.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Given this sequence-------\n",
      "\n",
      "lans</username>\n",
      "        <id>94325</id>\n",
      "      </contributor>\n",
      "      <comment>update redirect</comment>\n",
      "      <text xml:space=\"preserve\">#REDIRECT[[History of Poland (1939â1945)]]</text>\n",
      "    </revision>\n",
      "  </page>\n",
      "  <page>\n",
      "    <title>Haddocks eyes</title>\n",
      "  \n",
      "\n",
      "-------the transformer continued with\n",
      "\n",
      "  <id>41547</id>     <revision>       <id>419004907</id>       <timestamp>2006-12-12T12:13:23Z</timestamp>       <contributor>         <username>Johann III</username>         <id>68988</id>       </contributor>       <minor />       <comment>/* Icon screed to [[Adam IIII of III of IIIII]]</comment>       <text xml:space=\"preserve\">#REDIII III - [[Icon II of III]]</comment>       <text xml:space=\"preserve\">#REDIII [[Icon III of III II]]</text>     </revision>   </page>    <page>     <title>Control III s a Control III 1507</title>     <id>8758</id>     <revision>       <id>41782130</id>       <t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|█████████████████████████████████████████████████████████████▉                        | 36001/50000 [1:03:09<7:40:55,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|██████████████████████████████████████████████████████████████████▉                     | 38000/50000 [1:06:33<20:45,  9.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Given this sequence-------\n",
      "\n",
      "ll too many emigrants, already weak, some with [[cholera]], died during the passage to North America. \n",
      "\n",
      "Ireland experienced a massive number of evictions, due to financial reasons, and infamously to 'clear' their lands to allow cattle grazing, similar to t\n",
      "\n",
      "-------the transformer continued with\n",
      "\n",
      "he company of the public majority in the end of format their terms are with the them of the charage of the company of the strong and the first of the public complete.  The cattle of the perhaps of the perhaps of the perhaps of the majority of the strong and the particle of the law, and the extent the tortal particle of the cattle is the defeat the perhaps of the complete of the one of the complete in the particle of the work of the substantial particle. The only perhaps is also the present of the [[actle theories]] of the [[complete defensive]] on the body of the present of the side of the [[p"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|█████████████████████████████████████████████████████████████████▎                    | 38001/50000 [1:06:39<6:26:58,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|██████████████████████████████████████████████████████████████████████▍                 | 40000/50000 [1:10:03<17:28,  9.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Given this sequence-------\n",
      "\n",
      " of the dress. Likewise, numerous Slavic sources from the 15th century onwards refer to the &quot;Jewish Kaftan&quot;. The Tsarist edict of the mid-19th century banning Jewish outfits mentions the &quot;Jewish Kaftan&quot; and &quot;Jewish hat&quot; - as a\n",
      "\n",
      "-------the transformer continued with\n",
      "\n",
      " place of the interest for his world. This world with the basist banning start banning specialized the world as the town of the theory of the continued to the precise of the continued to leave the more of the death, which the real format the start to the basist to basis of the [[basis of the Sands]]. After the early the large of the one of the basists of the first to the two of the start of the throne of the starting the money of the first had as a number of the money of the [[North of the Paper]] and [[1920]]. However, the start is often the [[North Holy of the Start of the North Acts of the "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████████████████████████████████████████████████████████████████▊                 | 40001/50000 [1:10:09<5:24:00,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|█████████████████████████████████████████████████████████████████████████▉              | 42000/50000 [1:13:32<13:48,  9.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Given this sequence-------\n",
      "\n",
      "isabilities of blindness, deafness and a severe speech impediment.\n",
      "\n",
      "In the comedy cartoon series [[South Park]] Helen Keller's life was shown in a musical.\n",
      "\n",
      "Her life and achievements are celebrated annually in [[Tuscumbia, Alabama]], her hometown, in the [\n",
      "\n",
      "-------the transformer continued with\n",
      "\n",
      "[United Kingdom]] and [[United States]] and [[United Kingdom]]. The recording the [[United States]] and other that the common source the activity of the consensus of principles in the common of [[United States]] and [[Hebrew Year]] and [[United States]].  ====History=== '''The Saur of the Devil Saur Works''' is a [[United States]] and [[United States]] and [[1990s]] and [[1990s]] and [[United States]] and [[United States]] and [[United States]] and [[United States]] and [[United States]] and [[United States]]. The [[United States of Saurres]] and [[United States]] and [[United States]] and [[U"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████████████████████████████████████████████████████████████████████▏             | 42001/50000 [1:13:38<4:18:47,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|█████████████████████████████████████████████████████████████████████████████▍          | 44000/50000 [1:17:02<10:29,  9.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Given this sequence-------\n",
      "\n",
      "Code of Hammurabi]]. This was written on a [[stela]] and placed in a public place, so that all could see it, even though very few could read. This stela was removed as plunder to the Elamite capital [[Susa]], where it was rediscovered in [[1901]], and it n\n",
      "\n",
      "-------the transformer continued with\n",
      "\n",
      "ot appearing the appears of the could narrow of the could in [[1919]]. The could not record to the experiment of the architecture of the word in the order of the experience of the state of the support of the sound of the single the provided must of the second of the [[American America]]. The the term of the state of the the reason that the content is a record of the order that the over the proposed medical of the [[Ward of the content|the Contention of the Contention of the United States]].  The new considered the exploit of the [[Contention of Contention]] and the [[Lee of Contention of Conte"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|███████████████████████████████████████████████████████████████████████████▋          | 44001/50000 [1:17:08<3:14:49,  1.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|████████████████████████████████████████████████████████████████████████████████▉       | 46000/50000 [1:20:32<06:41,  9.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Given this sequence-------\n",
      "\n",
      "]] Dayton Scale Division is sold to the [[Hobart Manufacturing Company]].\n",
      "* [[1942]] Ticketograph Division is sold to the [[National Postal Meter Company]].  \n",
      "* [[1958]] Time Equipment Division is sold to the [[SimplexGrinnell|Simplex Time Recorder Company\n",
      "\n",
      "-------the transformer continued with\n",
      "\n",
      "]]. * [[1940]] - [[Canada Company]], American company and [[1914]] and [[1940]] and [[1931]] and [[1960]] authority in [[1975]]. * [[1971]] - [[France Hong]], American sound [[1931]] - [[1953]] - [[John Company|John Company]], [[San Village|Domund]] and [[1942]], and [[1883]] and [[1966]] authority and [[Hong River]]. * [[1941]] - [[1960]] - [[John Romer]], American characterized by [[1931]] by [[1872]] by [[Rome March]], [[1965]] by [[Jersey Romere]]. American authority of the [[President Romer (commercial properties)|Romerent Romer]] and [[Philis Romere]] and [[1936]] by [[1912]]. The [[Sama"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|███████████████████████████████████████████████████████████████████████████████       | 46001/50000 [1:20:38<1:54:00,  1.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|████████████████████████████████████████████████████████████████████████████████████▍   | 48000/50000 [1:24:02<03:25,  9.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Given this sequence-------\n",
      "\n",
      "trasts with the much more adversarial nature of the UK [[parliament]].\n",
      "\n",
      "One political party, [[Mec Vannin]], advocates the establishment of a sovereign [[republic]]. A [[Manx Labour Party]] also exists, unaffiliated to the UK [[Labour Party (UK)|Labour Par\n",
      "\n",
      "-------the transformer continued with\n",
      "\n",
      "ty]], [[Labour Party]], [[Party Party]], [[Ukrid Party]], [[Party Party Party]], [[Party Party]], [[Party Party]], [[Party Party Party]], [[Party Party Party Party]], [[Party Party Party]], [[Party Party Party]], [[Party Party]], [[Party Party Party]], [[Party Party]], [[Party Party Party|Party Party]], [[Party Party Party]], [[Party Party Party|Party Party Party]], [[Party Party]], [[Party Party Party]], [[Party Party Party]], [[Party Party Party]], [[Party Party Party Party]], [[Party Party Party]], [[Party Party Party]], [[Party Party Party]], [[Party Party Party]], [[Party Party Party Part"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|██████████████████████████████████████████████████████████████████████████████████▌   | 48001/50000 [1:24:08<1:05:05,  1.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████▉| 49999/50000 [1:27:31<00:00,  9.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Given this sequence-------\n",
      "\n",
      " of Germany|German flag]] on the arm; when he took it off, the pale [[khaki]] shirt underneath showed a [[Wehrmacht]] national emblem on the collar and a [[swastika]] armband. A reporter from ''[[The Sun (newspaper)|The Sun]]'' who had been tipped off prev\n",
      "\n",
      "-------the transformer continued with\n",
      "\n",
      "ious in [[1945]] to [[1941]] the [[1915]] to 10 showed the [[Sun Bonn]].  The [[1920s]] and [[1944]] and [[1917]], and the [[March Bonnesser Sun Anders Comparison]] in [[1931]], and the ''[[March Bonnesser Bonnesser Bonnesser]]'' and [[1941]] and [[1985]].  It was a comparisoned of the [[Simon Bonnesser Comparison]] and [[Canada Bonnesser Comparison]] and [[Bonnesser Bonnesser]] and [[1950]] and the [[March 1931]] in [[1997]].  For example the [[Persian British Comparison]] and [[1954]] the [[1940s]] of [[1980]].  At the [[National Theology]] is a [[International Division]] in [[1976]].  The ["
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 50000/50000 [1:27:38<00:00,  9.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training took 1.46 hours.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class Args:\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "args = Args(num_batches=50_000,  # 1_000_000,\n",
    "            batch_size=32,\n",
    "            data=None,\n",
    "            lr=0.0001,\n",
    "            tb_dir='./runs',\n",
    "            embedding_size=128,\n",
    "            num_heads=8,\n",
    "            context=256,\n",
    "            depth=12, \n",
    "            seed=1, \n",
    "            test_every=2000, \n",
    "            test_subset=100000, \n",
    "            test_batchsize=64, \n",
    "            gradient_clipping=1.0, \n",
    "            lr_warmup=5000, \n",
    "            sample_length=600, \n",
    "            attention_type=\"default\",\n",
    "            final=False)  # \"store_true\")\n",
    "\n",
    "start_time = time.time()\n",
    "create_train_test(args)\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f'Training took {elapsed_time/60/60:.2f} hours.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d881e65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
