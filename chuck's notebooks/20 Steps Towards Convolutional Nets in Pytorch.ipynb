{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Convolutional Neural Network in Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a convolutional neural network for classifying MNIST digits.  \n",
    "\n",
    "Each image is 28 x 28.  Each pixel just has one intensity, no colors.  The number of values in each pixel is called the number of channels.  Our black and white, or grayscale, images have one channel. Color images have 3 channels, for red, green, and blue values.  Pytorch assumes the channels are in the first dimension, so our MNIST digits are each of size 1 x 28 x 28.\n",
    "\n",
    "Let's create a torch tensor or zeros with the shape of two MNIST digit images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 28, 28])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.zeros((2, 1, 28, 28))\n",
    "z.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a layer of 5 convolutional units in Pytorch.  If we want to apply each unit to all image patches that are 4 x 4, we need to specify the kernel size to be 4 and the stride to be 1.\n",
    "\n",
    "The first argument to `torch.nn.Conv2d` is the number of channels in the input images, which is 1 in our case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(1, 5, kernel_size=(4, 4), stride=(1, 1))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer1 = torch.nn.Conv2d(1, 5, 4, 1)\n",
    "layer1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can pass our two images in `z` through this layer to see what the size of the result it.\n",
    "\n",
    "`torch.nn` objects can act as functions, because they have implemented the `__call__` function. See [this explanation](https://realpython.com/python-callable-instances/) from [Real Python](https://realpython.com/f)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 25, 25])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_layer1 = layer1(z)\n",
    "output_layer1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does this shape make sense?\n",
    "* 2 is the number of samples in `z`,\n",
    "* 5 is the number of units in this layer,\n",
    "* 25, 25 is the shape (25 x 25) of the resulting \"image\" produced by each of the 5 units.\n",
    "\n",
    "How can we calculate the 25?  Easy. We are striding by 1 with kernel size 4, so once we have used 28 - 4 columns, we have one more patch to make.  So we get 28 - 4 + 1, or 25.\n",
    "\n",
    "However, if we stride by 2, then we must divide by 2, to get  (28 - 4) // 2 + 1, or 13.  \n",
    "\n",
    "Let's check this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 13, 13])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer1 = torch.nn.Conv2d(1, 5, 4, 2)\n",
    "output_layer1 = layer1(z)\n",
    "output_layer1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay.  We are correct!\n",
    "\n",
    "In general, the calculation will be\n",
    "\n",
    "        (input_size - kernel_size) // 2 + stride"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we want another convolutional layer, say with 4 units, kernel size of 3, and stride of 1?\n",
    "\n",
    "        layer2 = torch.nn.Conv2d(?, 4, 3, 1)\n",
    "\n",
    "What should we use for \"?\" ?   Remember, this first argument is the number of channels, or values, for each pixel. Our `layer1` has 5 units. Each of these 5 units produces a 13 x 13 image, so `layer2` gets inputs with 5 channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(5, 4, kernel_size=(3, 3), stride=(1, 1))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer2 = torch.nn.Conv2d(5, 4, 3, 1)\n",
    "layer2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 11, 11])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_layer2 = layer2(layer1(z))\n",
    "output_layer2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Humm...why is the result for each unit in this layer an 11 x 11 image?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(13 - 3) // 1 + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build multiple layers in Pytorch, we need to combine them in a data structure that Pytorch understands, and can do things like collect all of the weights from all of the layers to be used in its optimization functions.  We can do this with `torch.nn.Sequential`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(1, 5, kernel_size=(4, 4), stride=(2, 2))\n",
       "  (1): Conv2d(5, 4, kernel_size=(3, 3), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnet = torch.nn.Sequential(layer1, layer2)\n",
    "nnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 11, 11])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnet(z).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay. Let's review what we have just done, and use variables for the quantities we used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 11)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_input_channels = 1\n",
    "input_size_1 = 28\n",
    "n_units_1 = 5\n",
    "kernel_1 = 4\n",
    "stride_1 = 2\n",
    "\n",
    "n_units_2 = 4\n",
    "kernel_2 = 3\n",
    "stride_2 = 1\n",
    "\n",
    "output_size_1 = (input_size_1 - kernel_1) // stride_1 + 1\n",
    "output_size_2 = (output_size_1 - kernel_2) // stride_2 + 1\n",
    "\n",
    "output_size_1, output_size_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(1, 5, kernel_size=(4, 4), stride=(2, 2))\n",
       "  (1): Conv2d(5, 4, kernel_size=(3, 3), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnet = torch.nn.Sequential(torch.nn.Conv2d(n_input_channels, n_units_1, kernel_1, stride_1),\n",
    "                           torch.nn.Conv2d(n_units_1, n_units_2, kernel_2, stride_2))\n",
    "nnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 1, 28, 28]), torch.Size([2, 4, 11, 11]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.shape, nnet(z).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want to classify MNIST digits, we need an output layer containing 10 units, each one connected to all of the output values from the second convolutional layer.  We can specifiy that with\n",
    "\n",
    "        torch.nn.Linear(n_inputs, n_outputs)\n",
    "\n",
    "but for this we need to know how many inputs we will have if we concatenate all of the outputs of the second convolutional layer for each MNIST digit into a one-dimensional vector.  How many will this be?\n",
    "\n",
    "Well, let's see.  The second convolutional layer outputs an 11 x 11 image for each of its units, and the layer has 4 units.  So this will result in 11 * 11 * 4, or 484 values.  This is usually called a fully-connected, or dense, layer. Okay.  We can just append this new layer onto our `torch.nn.Sequential` list.\n",
    "\n",
    "BUT, first we must add a component that actually does the concatenation for us, or, in Pytorch jargon, that flattens it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "484"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "11 * 11 * 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_dim\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_dim\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Flattens a contiguous range of dims into a tensor. For use with :class:`~nn.Sequential`.\n",
       "See :meth:`torch.flatten` for details.\n",
       "\n",
       "Shape:\n",
       "    - Input: :math:`(*, S_{\\text{start}},..., S_{i}, ..., S_{\\text{end}}, *)`,'\n",
       "      where :math:`S_{i}` is the size at dimension :math:`i` and :math:`*` means any\n",
       "      number of dimensions including none.\n",
       "    - Output: :math:`(*, \\prod_{i=\\text{start}}^{\\text{end}} S_{i}, *)`.\n",
       "\n",
       "Args:\n",
       "    start_dim: first dim to flatten (default = 1).\n",
       "    end_dim: last dim to flatten (default = -1).\n",
       "\n",
       "Examples::\n",
       "    >>> input = torch.randn(32, 1, 5, 5)\n",
       "    >>> # With default parameters\n",
       "    >>> m = nn.Flatten()\n",
       "    >>> output = m(input)\n",
       "    >>> output.size()\n",
       "    torch.Size([32, 25])\n",
       "    >>> # With non-default parameters\n",
       "    >>> m = nn.Flatten(0, 2)\n",
       "    >>> output = m(input)\n",
       "    >>> output.size()\n",
       "    torch.Size([160, 5])\n",
       "\u001b[0;31mInit docstring:\u001b[0m Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
       "\u001b[0;31mFile:\u001b[0m           ~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/flatten.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.nn.Flatten?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(1, 5, kernel_size=(4, 4), stride=(2, 2))\n",
       "  (1): Conv2d(5, 4, kernel_size=(3, 3), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 11, 11])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnet(z).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(1, 5, kernel_size=(4, 4), stride=(2, 2))\n",
       "  (1): Conv2d(5, 4, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (2): Flatten(start_dim=1, end_dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnet.append(torch.nn.Flatten())\n",
    "nnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 484])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnet(z).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool!  It left the first dimension (index 0) alone.  Pytorch assumes the first dimension is for samples. We don't want to flatten that one, but we do want to flatten all others. So this flattened the 4 x 11 x 11 arrays into 484-dimensional vectors.  Just what we want as input to our fully-connected layer.\n",
    "\n",
    "Now we can add our `torch.nn.Linear` layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(1, 5, kernel_size=(4, 4), stride=(2, 2))\n",
       "  (1): Conv2d(5, 4, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (2): Flatten(start_dim=1, end_dim=-1)\n",
       "  (3): Linear(in_features=484, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnet.append(torch.nn.Linear(484, 10))\n",
    "nnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What shape will our output be?  What do we want it to be?\n",
    "\n",
    "Well, the input to our network is 2, 28 x 28, digit images.  So we want one output value for each of the 10 digit classes, for each of the 2 images, or a 2 x 10 output tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnet(z).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want two fully-connected layers after the convolutional layers, we will need to add a nonlinear activation function.  We can use\n",
    "\n",
    "        torch.nn.Tanh()\n",
    "\n",
    "for example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay.  Let's do this, starting again with the quantities specified in variables.  Let's put 20 units in the first fully-connected layer, which will be `layer_3` now, and our 10-unit output layer will be `layer_4`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_input_channels = 1\n",
    "input_size_1 = 28\n",
    "n_units_1 = 5\n",
    "kernel_1 = 4\n",
    "stride_1 = 2\n",
    "\n",
    "n_units_2 = 4\n",
    "kernel_2 = 3\n",
    "stride_2 = 1\n",
    "\n",
    "output_size_1 = (input_size_1 - kernel_1) // stride_1 + 1\n",
    "output_size_2 = (output_size_1 - kernel_2) // stride_2 + 1\n",
    "\n",
    "n_units_3 = 20\n",
    "n_units_4 = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(1, 5, kernel_size=(4, 4), stride=(2, 2))\n",
       "  (1): Conv2d(5, 4, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (2): Flatten(start_dim=1, end_dim=-1)\n",
       "  (3): Linear(in_features=484, out_features=20, bias=True)\n",
       "  (4): Tanh()\n",
       "  (5): Linear(in_features=20, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnet = torch.nn.Sequential(torch.nn.Conv2d(n_input_channels, n_units_1, kernel_1, stride_1),\n",
    "                           torch.nn.Conv2d(n_units_1, n_units_2, kernel_2, stride_2),\n",
    "                           torch.nn.Flatten(),\n",
    "                           torch.nn.Linear(output_size_2 ** 2 * n_units_2, n_units_3),\n",
    "                           torch.nn.Tanh(),\n",
    "                           torch.nn.Linear(n_units_3, n_units_4))\n",
    "nnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 1, 28, 28]), torch.Size([2, 10]))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.shape, nnet(z).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, we made the net.  Now how do we train it?\n",
    "\n",
    "Relatively easily!  We will use the pytorch autograd, loss, and optimize facilities!!\n",
    "\n",
    "Pytorch provides a number of loss functions. We can use `torch.nn.NLLLoss` which you already know about from implementing the `neg_log_likelihood` function in assignments.  We can also use `torch.nn.CrossEntropyLoss`.  The first one assumes you have used a softmax (actually log softmax) as an activation function on the output layer.  The second one calculates this for us, so the simple linear output layer is all we need.  \n",
    "\n",
    "See [PyTorch CrossEntropyLoss vs. NLLLoss](https://jamesmccaffrey.wordpress.com/2020/06/11/pytorch-crossentropyloss-vs-nllloss-cross-entropy-loss-vs-negative-log-likelihood-loss) for a little more information, and [this Wikipedia page](https://en.wikipedia.org/wiki/Cross-entropy) about cross entropy in general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(1, 5, kernel_size=(4, 4), stride=(2, 2))\n",
       "  (1): Conv2d(5, 4, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (2): Flatten(start_dim=1, end_dim=-1)\n",
       "  (3): Linear(in_features=484, out_features=20, bias=True)\n",
       "  (4): Tanh()\n",
       "  (5): Linear(in_features=20, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percent_correct(Yclasses, T):\n",
    "    return (Yclasses == T).float().mean().item() * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train(nnet, X, T, n_epochs, learning_rate):\n",
    "    \n",
    "    optimizer = torch.optim.Adam(nnet.parameters(), lr=learning_rate)\n",
    "    loss_func = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "    \n",
    "        Y = nnet(X)\n",
    "        \n",
    "        loss = loss_func(Y, T)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step() \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        pc_train = percent_correct(use(nnet, Xtrain), Ttrain)\n",
    "        pc_val = percent_correct(use(nnet, Xval), Tval)\n",
    "        pc_test = percent_correct(use(nnet, Xtest), Ttest)\n",
    "        \n",
    "        print(f'Epoch {epoch + 1} %correct: Train {pc_train:.1f} Val {pc_val:.1f} Test {pc_test:.1f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "argmax(input) -> LongTensor\n",
       "\n",
       "Returns the indices of the maximum value of all elements in the :attr:`input` tensor.\n",
       "\n",
       "This is the second value returned by :meth:`torch.max`. See its\n",
       "documentation for the exact semantics of this method.\n",
       "\n",
       ".. note:: If there are multiple maximal values then the indices of the first maximal value are returned.\n",
       "\n",
       "Args:\n",
       "    input (Tensor): the input tensor.\n",
       "\n",
       "Example::\n",
       "\n",
       "    >>> a = torch.randn(4, 4)\n",
       "    >>> a\n",
       "    tensor([[ 1.3398,  0.2663, -0.2686,  0.2450],\n",
       "            [-0.7401, -0.8805, -0.3402, -1.1936],\n",
       "            [ 0.4907, -1.3948, -1.0691, -0.3132],\n",
       "            [-1.6092,  0.5419, -0.2993,  0.3195]])\n",
       "    >>> torch.argmax(a)\n",
       "    tensor(0)\n",
       "\n",
       ".. function:: argmax(input, dim, keepdim=False) -> LongTensor\n",
       "   :noindex:\n",
       "\n",
       "Returns the indices of the maximum values of a tensor across a dimension.\n",
       "\n",
       "This is the second value returned by :meth:`torch.max`. See its\n",
       "documentation for the exact semantics of this method.\n",
       "\n",
       "Args:\n",
       "    input (Tensor): the input tensor.\n",
       "    dim (int): the dimension to reduce. If ``None``, the argmax of the flattened input is returned.\n",
       "    keepdim (bool): whether the output tensor has :attr:`dim` retained or not. Ignored if ``dim=None``.\n",
       "\n",
       "Example::\n",
       "\n",
       "    >>> a = torch.randn(4, 4)\n",
       "    >>> a\n",
       "    tensor([[ 1.3398,  0.2663, -0.2686,  0.2450],\n",
       "            [-0.7401, -0.8805, -0.3402, -1.1936],\n",
       "            [ 0.4907, -1.3948, -1.0691, -0.3132],\n",
       "            [-1.6092,  0.5419, -0.2993,  0.3195]])\n",
       "    >>> torch.argmax(a, dim=1)\n",
       "    tensor([ 0,  2,  0,  1])\n",
       "\u001b[0;31mType:\u001b[0m      builtin_function_or_method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.argmax?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def use(nnet, X):\n",
    "    Y = nnet(X)\n",
    "    class_index = torch.argmax(Y, dim=1)  # not axis=1 as we did in numpy!\n",
    "    return class_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xtrain.shape=torch.Size([50000, 1, 28, 28]) Ttrain.shape=torch.Size([50000])\n",
      "    Xtrain.dtype=torch.float32 Ttrain.dtype=torch.int64\n",
      "Xval.shape=torch.Size([10000, 1, 28, 28]) Tval.shape=torch.Size([10000])\n",
      "   Xval.dtype=torch.float32 Tval.dtype=torch.int64\n",
      "Xtest.shape=torch.Size([10000, 1, 28, 28]) Ttest.shape=torch.Size([10000])\n",
      "   Xtest.dtype=torch.float32 Ttest.dtype=torch.int64\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "\n",
    "with gzip.open('mnist.pkl.gz', 'rb') as f:\n",
    "    train_set, valid_set, test_set = pickle.load(f, encoding='latin1')\n",
    "\n",
    "def X_as_torch(X):\n",
    "    return torch.from_numpy(X.reshape(-1, 1, 28, 28).astype(np.float32))\n",
    "def T_as_torch(T):\n",
    "    return torch.from_numpy(T.astype(np.int64))\n",
    "    \n",
    "Xtrain = X_as_torch(train_set[0])\n",
    "Ttrain = T_as_torch(train_set[1])\n",
    "\n",
    "Xval = X_as_torch(valid_set[0])\n",
    "Tval = T_as_torch(valid_set[1])\n",
    "\n",
    "Xtest = X_as_torch(test_set[0])\n",
    "Ttest = T_as_torch(test_set[1])\n",
    "\n",
    "print(f'{Xtrain.shape=} {Ttrain.shape=}\\n    {Xtrain.dtype=} {Ttrain.dtype=}')\n",
    "print(f'{Xval.shape=} {Tval.shape=}\\n   {Xval.dtype=} {Tval.dtype=}')\n",
    "print(f'{Xtest.shape=} {Ttest.shape=}\\n   {Xtest.dtype=} {Ttest.dtype=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.289999842643738"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percent_correct(use(nnet, Xval), Tval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 %correct: Train 10.0 Val 10.4 Test 10.1\n",
      "Epoch 2 %correct: Train 10.4 Val 10.8 Test 10.5\n",
      "Epoch 3 %correct: Train 12.6 Val 12.0 Test 12.6\n",
      "Epoch 4 %correct: Train 11.4 Val 10.7 Test 11.4\n",
      "Epoch 5 %correct: Train 11.4 Val 10.7 Test 11.4\n",
      "Epoch 6 %correct: Train 11.5 Val 10.8 Test 11.4\n",
      "Epoch 7 %correct: Train 10.2 Val 10.1 Test 10.5\n",
      "Epoch 8 %correct: Train 14.8 Val 15.0 Test 14.9\n",
      "Epoch 9 %correct: Train 15.3 Val 15.6 Test 15.5\n",
      "Epoch 10 %correct: Train 15.7 Val 15.8 Test 15.8\n"
     ]
    }
   ],
   "source": [
    "train(nnet, Xtrain, Ttrain, 10, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_input_channels = 1\n",
    "input_size_1 = 28\n",
    "n_units_1 = 50\n",
    "kernel_1 = 4\n",
    "stride_1 = 2\n",
    "\n",
    "n_units_2 = 20\n",
    "kernel_2 = 3\n",
    "stride_2 = 1\n",
    "\n",
    "output_size_1 = (input_size_1 - kernel_1) // stride_1 + 1\n",
    "output_size_2 = (output_size_1 - kernel_2) // stride_2 + 1\n",
    "\n",
    "n_units_3 = 20\n",
    "n_units_4 = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(1, 50, kernel_size=(4, 4), stride=(2, 2))\n",
       "  (1): Conv2d(50, 20, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (2): Flatten(start_dim=1, end_dim=-1)\n",
       "  (3): Linear(in_features=2420, out_features=20, bias=True)\n",
       "  (4): Tanh()\n",
       "  (5): Linear(in_features=20, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnet = torch.nn.Sequential(torch.nn.Conv2d(n_input_channels, n_units_1, kernel_1, stride_1),\n",
    "                           torch.nn.Conv2d(n_units_1, n_units_2, kernel_2, stride_2),\n",
    "                           torch.nn.Flatten(),\n",
    "                           torch.nn.Linear(output_size_2 ** 2 * n_units_2, n_units_3),\n",
    "                           torch.nn.Tanh(),\n",
    "                           torch.nn.Linear(n_units_3, n_units_4))\n",
    "nnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 %correct: Train 37.9 Val 38.7 Test 38.8\n",
      "Epoch 2 %correct: Train 61.9 Val 63.7 Test 62.9\n",
      "Epoch 3 %correct: Train 64.6 Val 66.6 Test 65.0\n",
      "Epoch 4 %correct: Train 66.7 Val 68.5 Test 66.8\n",
      "Epoch 5 %correct: Train 68.7 Val 70.4 Test 69.2\n",
      "Epoch 6 %correct: Train 69.5 Val 71.5 Test 70.6\n",
      "Epoch 7 %correct: Train 70.1 Val 71.9 Test 71.1\n",
      "Epoch 8 %correct: Train 71.2 Val 73.1 Test 72.1\n",
      "Epoch 9 %correct: Train 72.5 Val 74.6 Test 73.4\n",
      "Epoch 10 %correct: Train 73.9 Val 75.9 Test 74.8\n",
      "Epoch 11 %correct: Train 75.0 Val 77.2 Test 75.9\n",
      "Epoch 12 %correct: Train 76.0 Val 78.3 Test 77.0\n",
      "Epoch 13 %correct: Train 76.9 Val 79.4 Test 77.8\n",
      "Epoch 14 %correct: Train 77.8 Val 80.3 Test 78.5\n",
      "Epoch 15 %correct: Train 78.7 Val 81.0 Test 79.3\n",
      "Epoch 16 %correct: Train 79.4 Val 81.6 Test 79.9\n",
      "Epoch 17 %correct: Train 79.9 Val 82.2 Test 80.5\n",
      "Epoch 18 %correct: Train 80.4 Val 82.5 Test 81.1\n",
      "Epoch 19 %correct: Train 81.0 Val 83.2 Test 81.5\n",
      "Epoch 20 %correct: Train 81.4 Val 83.6 Test 82.1\n",
      "Epoch 21 %correct: Train 81.9 Val 84.1 Test 82.6\n",
      "Epoch 22 %correct: Train 82.4 Val 84.6 Test 82.9\n",
      "Epoch 23 %correct: Train 82.8 Val 85.0 Test 83.6\n",
      "Epoch 24 %correct: Train 83.2 Val 85.2 Test 83.9\n",
      "Epoch 25 %correct: Train 83.4 Val 85.4 Test 84.1\n",
      "Epoch 26 %correct: Train 83.6 Val 85.5 Test 84.3\n",
      "Epoch 27 %correct: Train 83.8 Val 85.7 Test 84.4\n",
      "Epoch 28 %correct: Train 84.0 Val 85.8 Test 84.5\n",
      "Epoch 29 %correct: Train 84.3 Val 86.0 Test 84.8\n",
      "Epoch 30 %correct: Train 84.5 Val 86.2 Test 85.1\n",
      "Epoch 31 %correct: Train 84.6 Val 86.4 Test 85.4\n",
      "Epoch 32 %correct: Train 84.8 Val 86.4 Test 85.7\n",
      "Epoch 33 %correct: Train 85.0 Val 86.7 Test 85.8\n",
      "Epoch 34 %correct: Train 85.2 Val 86.8 Test 86.0\n",
      "Epoch 35 %correct: Train 85.3 Val 87.0 Test 86.2\n",
      "Epoch 36 %correct: Train 85.5 Val 87.1 Test 86.4\n",
      "Epoch 37 %correct: Train 85.7 Val 87.2 Test 86.6\n",
      "Epoch 38 %correct: Train 85.8 Val 87.3 Test 86.8\n",
      "Epoch 39 %correct: Train 85.9 Val 87.3 Test 86.9\n",
      "Epoch 40 %correct: Train 86.0 Val 87.5 Test 87.1\n",
      "Epoch 41 %correct: Train 86.1 Val 87.7 Test 87.1\n",
      "Epoch 42 %correct: Train 86.2 Val 87.8 Test 87.2\n",
      "Epoch 43 %correct: Train 86.3 Val 87.9 Test 87.3\n",
      "Epoch 44 %correct: Train 86.5 Val 87.9 Test 87.4\n",
      "Epoch 45 %correct: Train 86.6 Val 88.1 Test 87.5\n",
      "Epoch 46 %correct: Train 86.7 Val 88.1 Test 87.6\n",
      "Epoch 47 %correct: Train 86.7 Val 88.1 Test 87.7\n",
      "Epoch 48 %correct: Train 86.9 Val 88.2 Test 87.7\n",
      "Epoch 49 %correct: Train 87.0 Val 88.2 Test 87.8\n",
      "Epoch 50 %correct: Train 87.0 Val 88.2 Test 87.9\n"
     ]
    }
   ],
   "source": [
    "train(nnet, Xtrain, Ttrain, 50, 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "If you were to put this code into a new `NeuralNetworkTorch` class, how would you specify the arguments to the constructor, so that the constructor can build the neural network?\n",
    "\n",
    "We need \n",
    "* the number of input channels and shape of the input images,\n",
    "* for each convolutional layer we need the number of units, kernel size, and stride,\n",
    "* for each fully-connected hidden layer we need number of units,\n",
    "* for the output layer we need the number of units.\n",
    "\n",
    "Seems easy enough---four arguments:\n",
    "* list of three ints, for the number of input channels and shape of the two-dimensional input images,\n",
    "* list of lists of ints, one sublist for each convolutional layer containing the number of units, kernel size, and stride,\n",
    "* list of ints for each fully-connected hidden layer we need number of units,\n",
    "* int, for the number of units in the output layer\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we will be wanting to see the outputs of each layer, we need a different structure to the net.  Let's define each layer separately and include the layer's activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_input_channels = 1\n",
    "input_size = 28\n",
    "n_units_1 = 50\n",
    "kernel_1 = 4\n",
    "stride_1 = 2\n",
    "\n",
    "n_units_2 = 20\n",
    "kernel_2 = 3\n",
    "stride_2 = 1\n",
    "\n",
    "output_size_1 = (input_size - kernel_1) // stride_1 + 1\n",
    "output_size_2 = (output_size_1 - kernel_2) // stride_2 + 1\n",
    "\n",
    "n_units_3 = 20\n",
    "n_units_4 = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_flattened_size(n_input_channels, input_size, conv_layers):\n",
    "    z = torch.zeros((1, n_input_channels, input_size, input_size))\n",
    "    out = conv_layers(z)\n",
    "    return \n",
    "    in_size = input_size\n",
    "    for conv in conv_layers:\n",
    "        z = conv(z)\n",
    "\n",
    "    out = conv_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layers = torch.nn.Sequential(\n",
    "    torch.nn.Sequential(torch.nn.Conv2d(n_input_channels, n_units_1, kernel_1, stride_1),\n",
    "                        torch.nn.Tanh()),\n",
    "    torch.nn.Sequential(torch.nn.Conv2d(n_units_1, n_units_2, kernel_2, stride_2),\n",
    "                        torch.nn.Tanh()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.zeros(1, 1, 28, 28)\n",
    "y = conv_layers(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 20, 11, 11])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2420])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.reshape(1, -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2420"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_size_2 ** 2 * n_units_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_layers = torch.nn.Sequential(\n",
    "            torch.nn.Sequential(torch.nn.Linear(output_size_2 ** 2 * n_units_2, n_units_3),\n",
    "                                torch.nn.Tanh()),\n",
    "            torch.nn.Linear(n_units_3, n_units_4))\n",
    "\n",
    "def forward(conv_layers, fc_layers, X):\n",
    "    n_samples = X.shape[0]\n",
    "    Zs = [X]\n",
    "    for conv_layer in conv_layers:\n",
    "        # Zs.append(activation_function(conv_layer(Ys[-1])))\n",
    "        Zs.append(conv_layer(Zs[-1]))\n",
    "\n",
    "    Zs[-1] = Zs[-1].reshape(n_samples, -1)\n",
    "            \n",
    "    for fc_layer in fc_layers:\n",
    "        Zs.append(fc_layer(Zs[-1]))\n",
    "        \n",
    "    return Zs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[[[-7.9822e-02, -2.3267e-01, -2.0843e-01, -9.5234e-02],\n",
       "           [ 9.9745e-02,  1.8729e-01,  3.8440e-02, -1.0873e-01],\n",
       "           [-9.7491e-02,  1.2590e-01,  7.6633e-03, -2.2126e-01],\n",
       "           [-2.8589e-02, -2.1833e-01, -1.1838e-01, -1.2064e-01]]],\n",
       " \n",
       " \n",
       "         [[[ 6.9133e-02, -2.3409e-01,  1.7648e-01,  6.2355e-02],\n",
       "           [-1.0319e-01,  1.9576e-01, -2.2655e-01,  2.1083e-02],\n",
       "           [-1.0862e-01,  2.4989e-01,  7.8159e-02, -1.6312e-01],\n",
       "           [-1.3490e-01, -1.4565e-01,  2.4302e-01, -1.3308e-01]]],\n",
       " \n",
       " \n",
       "         [[[ 2.0420e-01,  1.4778e-01, -4.4402e-02, -2.4015e-01],\n",
       "           [-2.7844e-02, -4.4514e-02, -1.5794e-01, -2.4825e-01],\n",
       "           [-1.5558e-01,  5.6637e-02, -1.1044e-01, -1.8110e-01],\n",
       "           [ 1.9186e-01,  2.2550e-01, -2.2078e-01,  1.1631e-01]]],\n",
       " \n",
       " \n",
       "         [[[ 1.2246e-01,  1.6026e-01,  4.6630e-02,  5.6052e-02],\n",
       "           [-2.0488e-01,  1.3242e-01,  2.4871e-01,  8.9424e-02],\n",
       "           [-1.3369e-02,  1.9422e-01,  1.6588e-01,  1.4137e-01],\n",
       "           [ 2.2642e-01, -2.2406e-01, -2.3552e-01, -5.9419e-02]]],\n",
       " \n",
       " \n",
       "         [[[-9.3115e-02,  7.5137e-02, -2.1142e-01, -2.0197e-01],\n",
       "           [ 1.2482e-01,  1.7977e-01, -2.1754e-01, -6.6178e-03],\n",
       "           [-1.2732e-01, -7.5518e-03,  1.1585e-01, -2.3445e-01],\n",
       "           [-8.3964e-02,  1.8680e-02, -5.3206e-02, -2.4583e-01]]],\n",
       " \n",
       " \n",
       "         [[[ 1.3719e-01,  1.9942e-01, -2.2308e-02,  1.0390e-01],\n",
       "           [-1.5572e-01, -3.9730e-02, -1.6483e-01, -1.3419e-01],\n",
       "           [-1.4717e-01, -7.7031e-02,  1.2576e-01,  1.9029e-01],\n",
       "           [-2.2286e-02, -3.4111e-02, -9.0598e-02, -6.2779e-02]]],\n",
       " \n",
       " \n",
       "         [[[-1.6227e-01, -1.9816e-02, -8.0433e-02,  3.8248e-02],\n",
       "           [ 3.9152e-02, -2.4716e-01,  2.4300e-01, -1.8790e-02],\n",
       "           [ 6.3993e-02,  1.6968e-02,  8.7450e-02, -1.3861e-01],\n",
       "           [ 2.0270e-01, -2.3376e-01, -2.0838e-01, -2.4770e-01]]],\n",
       " \n",
       " \n",
       "         [[[ 1.1170e-01, -2.4615e-02, -2.3341e-01,  1.8853e-02],\n",
       "           [ 1.1794e-02, -8.8322e-02,  9.0168e-02, -4.8730e-02],\n",
       "           [-1.7110e-01,  1.0817e-01, -8.9821e-02,  1.5921e-01],\n",
       "           [-2.1884e-01,  2.4027e-01,  4.3867e-02, -1.0920e-01]]],\n",
       " \n",
       " \n",
       "         [[[-1.5695e-01,  1.9711e-02, -2.3572e-01, -1.1700e-01],\n",
       "           [-1.1212e-01, -1.1678e-01, -4.9354e-02,  3.1102e-02],\n",
       "           [-4.0946e-02,  1.0526e-01,  1.0644e-01, -3.8634e-02],\n",
       "           [-1.3953e-02, -2.2961e-01, -2.2333e-01,  1.7898e-01]]],\n",
       " \n",
       " \n",
       "         [[[-1.5658e-01, -5.7302e-02,  2.2706e-01, -5.2216e-02],\n",
       "           [-3.4806e-03, -1.0571e-01,  8.6617e-02, -2.2390e-01],\n",
       "           [ 7.6002e-02,  7.2916e-02, -9.1061e-02,  2.9637e-02],\n",
       "           [ 1.1975e-01, -1.1367e-01, -1.6337e-01, -1.0975e-01]]],\n",
       " \n",
       " \n",
       "         [[[-1.5780e-01,  2.2609e-01,  5.7633e-02,  2.0534e-01],\n",
       "           [ 1.5257e-02,  2.9455e-02, -1.8565e-01, -1.4591e-01],\n",
       "           [ 1.5442e-02, -2.1780e-01, -1.7087e-01,  1.9481e-01],\n",
       "           [-2.3959e-01, -1.0786e-01, -1.5629e-01, -1.8888e-01]]],\n",
       " \n",
       " \n",
       "         [[[-8.7391e-02, -1.4312e-01, -1.4668e-01,  2.4553e-01],\n",
       "           [-2.4573e-01, -3.8268e-02, -1.1659e-01,  2.0293e-01],\n",
       "           [ 1.6417e-01,  8.1117e-02,  7.1016e-02, -1.0071e-01],\n",
       "           [ 3.5554e-02,  2.4745e-01,  1.5493e-01,  8.7798e-02]]],\n",
       " \n",
       " \n",
       "         [[[-3.6470e-02, -9.3236e-02, -7.5877e-02, -2.0326e-01],\n",
       "           [ 1.2797e-01,  6.5611e-02,  1.7100e-01,  9.4556e-02],\n",
       "           [-5.9677e-02,  5.8736e-02, -7.3485e-02, -2.0380e-01],\n",
       "           [-1.0032e-01,  1.7555e-01, -1.2997e-01,  1.8525e-02]]],\n",
       " \n",
       " \n",
       "         [[[ 1.8109e-01, -7.1629e-02,  1.0845e-01,  2.2525e-01],\n",
       "           [ 1.5346e-01,  1.8993e-01,  2.0103e-01, -6.9690e-02],\n",
       "           [-8.3659e-02, -1.6966e-01, -2.0098e-01, -1.4522e-01],\n",
       "           [-4.2307e-02,  1.0960e-01, -4.0363e-02,  1.9924e-01]]],\n",
       " \n",
       " \n",
       "         [[[ 6.1804e-04,  2.1666e-01,  7.0378e-03,  1.2352e-02],\n",
       "           [ 1.9360e-01,  1.9859e-01, -1.3953e-02, -8.1068e-02],\n",
       "           [ 1.1348e-01, -3.7828e-02,  1.4992e-01,  1.0812e-01],\n",
       "           [-4.1482e-02,  1.7099e-01,  1.0286e-01, -2.1602e-01]]],\n",
       " \n",
       " \n",
       "         [[[-1.5215e-01,  1.8251e-02, -1.4210e-01,  1.0365e-01],\n",
       "           [ 5.7142e-03, -2.2138e-01, -2.2313e-01, -2.3967e-01],\n",
       "           [ 2.4222e-01, -9.1856e-02,  1.3972e-01,  1.7835e-01],\n",
       "           [ 1.6293e-01,  1.9847e-01,  9.7015e-02, -2.0177e-01]]],\n",
       " \n",
       " \n",
       "         [[[ 2.0918e-01,  3.2282e-02, -2.0551e-01,  5.0445e-02],\n",
       "           [ 1.5691e-01,  1.5601e-01, -2.3765e-01, -2.3136e-01],\n",
       "           [ 1.7343e-01,  2.0129e-01,  6.3530e-03, -1.7589e-02],\n",
       "           [ 1.6762e-01,  1.1251e-01, -7.6552e-02, -2.4142e-01]]],\n",
       " \n",
       " \n",
       "         [[[ 8.3360e-02,  1.4815e-01, -5.8387e-02, -8.3506e-02],\n",
       "           [-2.6843e-02, -6.3684e-03, -2.3626e-02,  1.3053e-01],\n",
       "           [ 1.1484e-01, -6.2676e-02, -5.4207e-04, -9.9885e-03],\n",
       "           [ 8.4240e-02, -2.3516e-01, -3.3378e-02, -1.9261e-01]]],\n",
       " \n",
       " \n",
       "         [[[-1.9536e-01,  1.5118e-02,  5.5955e-03,  1.0965e-01],\n",
       "           [ 1.4583e-01,  1.8819e-01, -4.1172e-02, -2.3905e-01],\n",
       "           [ 1.7383e-01, -1.7039e-01, -5.3230e-02,  7.2166e-02],\n",
       "           [ 1.8941e-01,  1.7807e-02, -1.3547e-01, -1.1864e-01]]],\n",
       " \n",
       " \n",
       "         [[[ 7.6009e-02, -1.3193e-01, -1.6347e-02,  1.0638e-01],\n",
       "           [-7.0965e-02,  2.0975e-01,  2.4080e-01, -3.3970e-02],\n",
       "           [-2.5375e-02,  5.2119e-02, -1.4511e-01, -1.3832e-02],\n",
       "           [-3.8669e-02,  1.3026e-01, -1.2046e-01,  4.5747e-02]]],\n",
       " \n",
       " \n",
       "         [[[ 2.4892e-01,  1.4801e-01,  1.2547e-01, -4.8522e-02],\n",
       "           [-7.5478e-02,  1.9967e-01, -8.1048e-02,  1.6919e-01],\n",
       "           [ 1.3657e-01,  2.7254e-02, -3.0787e-02,  1.3464e-01],\n",
       "           [-5.2168e-02,  1.4766e-02,  4.1584e-02,  3.7000e-02]]],\n",
       " \n",
       " \n",
       "         [[[ 1.6101e-02, -2.3522e-01,  1.8813e-01, -1.7446e-01],\n",
       "           [ 2.0576e-01,  1.9260e-01, -1.0628e-01,  1.8304e-01],\n",
       "           [ 1.4665e-01,  7.4832e-02, -1.5857e-01,  1.8268e-01],\n",
       "           [-8.8733e-02, -1.4923e-01,  1.9380e-01, -1.2684e-01]]],\n",
       " \n",
       " \n",
       "         [[[-1.3655e-01,  7.7009e-02,  2.0018e-01, -7.4154e-03],\n",
       "           [-1.5161e-01, -2.4993e-01,  7.5145e-02,  7.5889e-02],\n",
       "           [ 4.9872e-02,  2.0223e-01,  2.0184e-01,  1.6912e-01],\n",
       "           [-1.5823e-01,  2.4097e-01, -1.1428e-01,  2.4724e-01]]],\n",
       " \n",
       " \n",
       "         [[[ 5.7988e-02,  6.5425e-02, -2.3611e-01, -2.0069e-01],\n",
       "           [-1.4751e-01, -1.2010e-01, -1.4028e-01,  7.5400e-02],\n",
       "           [-2.2336e-01,  2.6436e-02, -6.5792e-02,  1.4808e-01],\n",
       "           [-2.3016e-01,  1.8964e-01,  1.5272e-01, -4.0270e-02]]],\n",
       " \n",
       " \n",
       "         [[[ 1.8965e-01, -1.0440e-01,  2.5100e-02, -2.1025e-01],\n",
       "           [ 1.5546e-01, -5.5049e-02, -1.8119e-01, -5.7441e-02],\n",
       "           [-1.0076e-01, -1.4071e-01, -2.1334e-01, -3.4966e-02],\n",
       "           [ 1.7695e-01, -4.4937e-02, -2.6804e-02,  4.8994e-02]]],\n",
       " \n",
       " \n",
       "         [[[ 3.9697e-02, -5.0798e-03, -1.8687e-01, -1.8189e-01],\n",
       "           [ 1.2027e-01,  1.6874e-01,  1.8015e-01, -7.6048e-02],\n",
       "           [ 1.2792e-01, -2.2651e-01,  6.2572e-02,  8.7144e-02],\n",
       "           [-2.1218e-01,  1.8041e-01, -7.7749e-02, -2.2827e-01]]],\n",
       " \n",
       " \n",
       "         [[[ 1.9976e-01, -9.9872e-02, -8.9584e-02,  7.5028e-02],\n",
       "           [-8.8683e-02, -1.0518e-01, -2.2824e-01, -1.9197e-01],\n",
       "           [-2.2997e-01,  2.0047e-01,  1.6517e-01,  3.1403e-02],\n",
       "           [ 2.1304e-01,  1.0837e-01,  1.2990e-01,  1.0198e-01]]],\n",
       " \n",
       " \n",
       "         [[[-1.8350e-01, -7.7669e-02,  2.0493e-01, -4.4973e-02],\n",
       "           [ 2.2889e-01, -2.1731e-01,  7.7436e-02, -1.4774e-01],\n",
       "           [-9.5793e-02,  1.3076e-01, -8.2113e-02, -2.2150e-02],\n",
       "           [ 1.0425e-01, -1.0497e-01, -1.3306e-01, -1.3355e-01]]],\n",
       " \n",
       " \n",
       "         [[[ 2.3521e-01, -7.7291e-02, -1.4456e-01, -1.0966e-01],\n",
       "           [-2.1133e-01, -6.0051e-02,  2.3449e-01, -1.7433e-01],\n",
       "           [ 1.5841e-01,  1.4197e-01,  6.4053e-02,  6.0868e-02],\n",
       "           [ 2.0224e-01,  7.0736e-02, -2.3210e-01, -7.9440e-02]]],\n",
       " \n",
       " \n",
       "         [[[ 1.5564e-01,  7.5529e-02,  7.9731e-02,  9.9665e-03],\n",
       "           [-5.4471e-02, -4.9625e-03, -1.9050e-01,  1.8806e-01],\n",
       "           [ 5.4654e-02,  2.7104e-02, -1.1075e-02, -7.3064e-02],\n",
       "           [-6.7468e-02, -1.3841e-01,  2.1890e-01, -9.6983e-02]]],\n",
       " \n",
       " \n",
       "         [[[ 9.2791e-02,  1.4708e-01, -1.8082e-01,  6.1420e-02],\n",
       "           [-1.5167e-01,  2.4807e-01, -2.0808e-02,  1.9060e-01],\n",
       "           [-4.7404e-02, -1.1681e-01, -4.2313e-02,  3.3544e-02],\n",
       "           [ 7.0911e-02, -1.6325e-01,  1.0003e-02, -1.9313e-01]]],\n",
       " \n",
       " \n",
       "         [[[ 5.1102e-03, -1.3381e-01, -2.1294e-01, -4.0651e-03],\n",
       "           [-3.8252e-02, -5.6977e-02, -4.1873e-02, -3.3838e-02],\n",
       "           [ 1.0862e-01,  7.8515e-02,  1.1869e-01,  2.4129e-01],\n",
       "           [-5.0750e-02, -1.5756e-01,  1.8751e-01,  1.5875e-01]]],\n",
       " \n",
       " \n",
       "         [[[ 1.3000e-01, -1.6263e-01,  1.7846e-01,  1.7984e-02],\n",
       "           [-1.0426e-01, -3.0584e-02,  4.4032e-02,  3.4957e-02],\n",
       "           [-6.7204e-02,  1.7346e-01, -1.8457e-01,  7.5847e-02],\n",
       "           [-2.1071e-01,  7.8668e-02,  2.4392e-01,  1.0817e-02]]],\n",
       " \n",
       " \n",
       "         [[[-1.6172e-01,  1.8073e-01, -1.5149e-01,  7.4383e-02],\n",
       "           [ 1.0659e-02,  1.8756e-01,  1.8872e-01,  1.4721e-01],\n",
       "           [ 1.6976e-01, -2.6944e-03,  1.6261e-01, -2.6814e-02],\n",
       "           [-6.1926e-02,  1.7735e-01,  1.6591e-01, -1.2991e-01]]],\n",
       " \n",
       " \n",
       "         [[[-1.1884e-01,  7.6852e-02, -6.2298e-03,  1.3201e-01],\n",
       "           [ 2.2856e-01, -1.5110e-05,  2.8380e-02, -1.2200e-01],\n",
       "           [ 8.7247e-02,  8.8923e-02, -1.0340e-01, -2.2117e-01],\n",
       "           [ 2.8583e-02, -4.8656e-02, -1.3616e-01, -3.6813e-02]]],\n",
       " \n",
       " \n",
       "         [[[ 2.0048e-01,  4.0103e-02,  1.3203e-01, -2.1159e-01],\n",
       "           [-1.3494e-01,  1.8974e-01,  2.3236e-02, -2.1646e-02],\n",
       "           [-1.3376e-01, -9.2884e-03, -4.6724e-02, -3.8039e-02],\n",
       "           [-2.2596e-01, -2.1989e-02,  1.2556e-01, -6.5901e-02]]],\n",
       " \n",
       " \n",
       "         [[[ 1.0976e-01,  1.2657e-01, -4.8578e-02, -3.4553e-02],\n",
       "           [ 7.4383e-02, -3.0977e-02,  1.3496e-01, -9.6522e-02],\n",
       "           [-3.1280e-02,  2.0170e-01,  3.2136e-02,  2.0164e-01],\n",
       "           [-1.5442e-01,  1.4566e-01,  9.2118e-03, -1.4865e-01]]],\n",
       " \n",
       " \n",
       "         [[[ 2.0483e-02,  1.5185e-01, -6.2832e-02, -5.5584e-02],\n",
       "           [ 1.1941e-01,  3.9596e-02,  1.0089e-02,  1.6959e-01],\n",
       "           [-1.0380e-01, -2.0460e-01, -2.1904e-01, -4.6148e-02],\n",
       "           [ 2.9636e-03,  1.7148e-01, -1.9676e-01, -5.9573e-02]]],\n",
       " \n",
       " \n",
       "         [[[-1.0887e-01, -1.2079e-01,  5.8759e-02, -2.1867e-01],\n",
       "           [-7.8678e-02,  2.3869e-01, -1.9563e-01,  1.0550e-01],\n",
       "           [-3.7084e-02, -1.5415e-02, -1.4095e-01,  6.5777e-03],\n",
       "           [ 1.1040e-01, -1.4889e-01,  2.3266e-01,  2.3304e-01]]],\n",
       " \n",
       " \n",
       "         [[[ 3.7213e-02,  3.1588e-02, -1.3449e-01, -2.3295e-02],\n",
       "           [ 1.8920e-01,  6.6448e-02,  8.0996e-02,  5.2264e-03],\n",
       "           [-2.0177e-02, -8.6475e-03,  6.3221e-02,  1.6591e-01],\n",
       "           [-2.2731e-01,  1.8669e-01,  5.5622e-03, -2.6067e-02]]],\n",
       " \n",
       " \n",
       "         [[[-2.1372e-01, -2.0558e-01, -1.3716e-01,  2.3981e-01],\n",
       "           [-1.3166e-01, -2.4513e-01,  1.5351e-01, -2.0588e-01],\n",
       "           [ 1.1536e-01, -1.8181e-01,  2.2926e-01, -1.0781e-01],\n",
       "           [ 2.0880e-01,  7.8303e-03,  1.0544e-01,  5.8519e-02]]],\n",
       " \n",
       " \n",
       "         [[[ 1.6307e-01,  2.2766e-01,  3.8210e-02,  4.0741e-02],\n",
       "           [-1.1936e-02,  1.2918e-02,  2.3001e-01, -1.1354e-01],\n",
       "           [ 1.3213e-01, -2.3486e-01,  1.3066e-01, -1.7135e-01],\n",
       "           [-1.7132e-01, -1.0133e-01, -6.4269e-02, -9.4742e-02]]],\n",
       " \n",
       " \n",
       "         [[[-1.6358e-01, -1.9068e-01, -1.5905e-01, -2.4653e-01],\n",
       "           [-1.6120e-01, -1.7023e-01, -2.1937e-01, -1.7133e-01],\n",
       "           [-2.3525e-01,  9.9995e-02, -2.0503e-01,  1.7949e-01],\n",
       "           [ 1.2550e-01, -1.0669e-01, -2.2339e-01, -9.0266e-02]]],\n",
       " \n",
       " \n",
       "         [[[-1.1603e-01, -2.7027e-02, -1.9916e-01, -1.2247e-01],\n",
       "           [ 1.3540e-01,  1.4668e-01, -1.6536e-01,  5.4345e-02],\n",
       "           [-1.4189e-01, -1.5530e-01,  2.1915e-01, -2.1898e-01],\n",
       "           [ 2.1171e-01,  1.6511e-01, -4.0116e-02, -5.8795e-02]]],\n",
       " \n",
       " \n",
       "         [[[ 2.5984e-02,  1.0758e-01, -6.3978e-02,  1.1546e-01],\n",
       "           [ 2.0082e-01, -6.1105e-02,  1.6354e-01,  2.5562e-02],\n",
       "           [ 1.9114e-01, -1.4274e-01, -1.9564e-01,  3.1814e-02],\n",
       "           [ 7.6609e-02, -2.1390e-02,  1.9432e-01,  2.4204e-01]]],\n",
       " \n",
       " \n",
       "         [[[ 2.4937e-01,  4.1062e-02,  3.7381e-02, -4.2756e-02],\n",
       "           [ 7.4820e-03, -8.6167e-04, -2.3503e-01, -1.2506e-02],\n",
       "           [ 1.2343e-01,  1.2309e-02,  1.8765e-01, -1.6811e-02],\n",
       "           [-1.1359e-01, -8.6701e-02, -2.3466e-01, -2.4258e-02]]],\n",
       " \n",
       " \n",
       "         [[[ 1.5902e-01, -1.6249e-01,  2.1288e-02, -1.3019e-01],\n",
       "           [ 1.7136e-01,  3.0312e-02, -1.8704e-01, -3.6852e-02],\n",
       "           [-1.6768e-01,  2.0047e-01,  8.4966e-05, -5.0626e-02],\n",
       "           [ 6.2318e-02, -1.2309e-01, -1.1310e-01, -6.6392e-02]]],\n",
       " \n",
       " \n",
       "         [[[-3.0058e-02,  1.5028e-01, -2.3552e-01,  6.9774e-02],\n",
       "           [-2.2425e-01,  1.4472e-02,  1.9288e-01,  1.6469e-01],\n",
       "           [-2.0019e-01, -1.7981e-01,  3.5112e-02,  1.6762e-01],\n",
       "           [-1.7388e-01, -1.9845e-01,  5.1633e-02,  1.3192e-01]]],\n",
       " \n",
       " \n",
       "         [[[ 2.2647e-01, -1.5635e-01, -1.4168e-01,  5.5998e-02],\n",
       "           [-1.0931e-02,  9.4546e-02,  2.0969e-01,  1.6880e-01],\n",
       "           [-9.1051e-02,  7.4125e-02, -1.4314e-01,  7.9338e-02],\n",
       "           [-4.7956e-02,  7.0505e-02,  6.0961e-02, -2.3281e-01]]],\n",
       " \n",
       " \n",
       "         [[[-1.3857e-01, -1.4189e-01, -3.4321e-02,  7.4147e-02],\n",
       "           [-9.0970e-03,  8.6364e-02, -2.9142e-02, -1.0647e-03],\n",
       "           [-1.2042e-01,  8.9466e-02,  5.6819e-02,  2.1105e-01],\n",
       "           [ 3.5868e-02, -2.0769e-01, -1.8775e-02, -2.0348e-03]]]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.1765, -0.2222, -0.1452, -0.1672,  0.1448,  0.1885, -0.1912,  0.0144,\n",
       "         -0.1062,  0.2266,  0.0542, -0.1179, -0.1546, -0.0563,  0.1075,  0.0740,\n",
       "         -0.0635,  0.1891,  0.1234,  0.2344,  0.1617,  0.1206,  0.2115,  0.1819,\n",
       "         -0.2317, -0.1036,  0.2337, -0.1387,  0.0570, -0.2097, -0.0540, -0.0165,\n",
       "          0.0050, -0.0101, -0.1447, -0.0830,  0.0916, -0.0370, -0.0196, -0.1941,\n",
       "          0.2302, -0.1921,  0.0738, -0.1348,  0.1137, -0.0862, -0.1570,  0.0640,\n",
       "         -0.1189,  0.1439], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[[[-0.0053, -0.0082, -0.0270],\n",
       "           [-0.0370, -0.0343,  0.0101],\n",
       "           [-0.0431,  0.0093,  0.0177]],\n",
       " \n",
       "          [[ 0.0324,  0.0114,  0.0043],\n",
       "           [ 0.0215,  0.0021, -0.0015],\n",
       "           [ 0.0211, -0.0216,  0.0412]],\n",
       " \n",
       "          [[ 0.0383, -0.0184,  0.0436],\n",
       "           [ 0.0458,  0.0094,  0.0083],\n",
       "           [-0.0372, -0.0331,  0.0049]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 0.0281, -0.0105,  0.0444],\n",
       "           [ 0.0056, -0.0077, -0.0106],\n",
       "           [-0.0058, -0.0469, -0.0223]],\n",
       " \n",
       "          [[ 0.0382,  0.0277,  0.0465],\n",
       "           [ 0.0352, -0.0247, -0.0091],\n",
       "           [-0.0092, -0.0044,  0.0026]],\n",
       " \n",
       "          [[-0.0022,  0.0123,  0.0229],\n",
       "           [ 0.0387, -0.0178, -0.0467],\n",
       "           [-0.0019, -0.0184,  0.0135]]],\n",
       " \n",
       " \n",
       "         [[[ 0.0384, -0.0035,  0.0186],\n",
       "           [ 0.0205, -0.0289, -0.0128],\n",
       "           [-0.0051, -0.0366, -0.0161]],\n",
       " \n",
       "          [[ 0.0240, -0.0016, -0.0049],\n",
       "           [ 0.0380,  0.0075,  0.0005],\n",
       "           [ 0.0100,  0.0133,  0.0318]],\n",
       " \n",
       "          [[ 0.0424,  0.0175, -0.0181],\n",
       "           [-0.0152, -0.0127, -0.0329],\n",
       "           [ 0.0461, -0.0461, -0.0267]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-0.0019, -0.0318,  0.0390],\n",
       "           [ 0.0326, -0.0299, -0.0140],\n",
       "           [ 0.0298, -0.0137, -0.0314]],\n",
       " \n",
       "          [[ 0.0053, -0.0197,  0.0128],\n",
       "           [ 0.0205,  0.0053,  0.0125],\n",
       "           [ 0.0266, -0.0007,  0.0405]],\n",
       " \n",
       "          [[ 0.0022,  0.0427,  0.0159],\n",
       "           [-0.0293,  0.0384,  0.0418],\n",
       "           [ 0.0154, -0.0305,  0.0175]]],\n",
       " \n",
       " \n",
       "         [[[-0.0126,  0.0324, -0.0450],\n",
       "           [ 0.0090, -0.0451, -0.0434],\n",
       "           [-0.0051, -0.0290,  0.0170]],\n",
       " \n",
       "          [[-0.0016,  0.0356,  0.0312],\n",
       "           [-0.0321, -0.0405,  0.0244],\n",
       "           [-0.0168,  0.0045,  0.0342]],\n",
       " \n",
       "          [[-0.0101,  0.0191, -0.0173],\n",
       "           [ 0.0160, -0.0373, -0.0426],\n",
       "           [ 0.0174, -0.0157, -0.0044]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 0.0434,  0.0055,  0.0028],\n",
       "           [ 0.0402, -0.0208,  0.0241],\n",
       "           [-0.0246,  0.0115,  0.0150]],\n",
       " \n",
       "          [[ 0.0423,  0.0233,  0.0239],\n",
       "           [-0.0329,  0.0080,  0.0015],\n",
       "           [-0.0017,  0.0390,  0.0425]],\n",
       " \n",
       "          [[-0.0082, -0.0317,  0.0344],\n",
       "           [ 0.0436, -0.0054, -0.0297],\n",
       "           [-0.0447,  0.0116,  0.0021]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[-0.0228, -0.0016, -0.0363],\n",
       "           [-0.0465,  0.0100,  0.0046],\n",
       "           [ 0.0239,  0.0212,  0.0093]],\n",
       " \n",
       "          [[ 0.0097,  0.0206,  0.0075],\n",
       "           [-0.0355, -0.0036,  0.0035],\n",
       "           [ 0.0346,  0.0418, -0.0309]],\n",
       " \n",
       "          [[ 0.0346, -0.0335,  0.0325],\n",
       "           [ 0.0334, -0.0358,  0.0288],\n",
       "           [-0.0412, -0.0151,  0.0273]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-0.0462,  0.0435, -0.0399],\n",
       "           [-0.0404, -0.0014, -0.0073],\n",
       "           [-0.0433, -0.0071, -0.0068]],\n",
       " \n",
       "          [[-0.0311, -0.0248, -0.0340],\n",
       "           [-0.0124,  0.0240, -0.0342],\n",
       "           [ 0.0425,  0.0287, -0.0172]],\n",
       " \n",
       "          [[-0.0284, -0.0308,  0.0250],\n",
       "           [ 0.0421, -0.0362, -0.0258],\n",
       "           [ 0.0148, -0.0298, -0.0349]]],\n",
       " \n",
       " \n",
       "         [[[ 0.0095, -0.0136, -0.0441],\n",
       "           [-0.0069, -0.0161, -0.0375],\n",
       "           [-0.0443, -0.0227,  0.0237]],\n",
       " \n",
       "          [[-0.0374, -0.0097, -0.0198],\n",
       "           [ 0.0407, -0.0284,  0.0045],\n",
       "           [-0.0043,  0.0303,  0.0280]],\n",
       " \n",
       "          [[ 0.0240,  0.0199, -0.0318],\n",
       "           [ 0.0416,  0.0160, -0.0191],\n",
       "           [ 0.0151,  0.0411, -0.0344]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 0.0333,  0.0005, -0.0004],\n",
       "           [-0.0339,  0.0043, -0.0382],\n",
       "           [ 0.0030, -0.0168,  0.0005]],\n",
       " \n",
       "          [[ 0.0302,  0.0304,  0.0041],\n",
       "           [ 0.0095,  0.0020,  0.0200],\n",
       "           [-0.0065,  0.0105,  0.0321]],\n",
       " \n",
       "          [[-0.0440,  0.0220, -0.0222],\n",
       "           [-0.0299,  0.0253, -0.0416],\n",
       "           [-0.0387,  0.0099, -0.0117]]],\n",
       " \n",
       " \n",
       "         [[[ 0.0057,  0.0180,  0.0335],\n",
       "           [-0.0080,  0.0330, -0.0146],\n",
       "           [ 0.0057,  0.0098,  0.0228]],\n",
       " \n",
       "          [[ 0.0030,  0.0406,  0.0349],\n",
       "           [-0.0373, -0.0107, -0.0459],\n",
       "           [-0.0190,  0.0433,  0.0105]],\n",
       " \n",
       "          [[ 0.0458,  0.0330,  0.0228],\n",
       "           [-0.0111, -0.0186,  0.0451],\n",
       "           [-0.0291,  0.0078, -0.0343]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-0.0294, -0.0012,  0.0303],\n",
       "           [-0.0394, -0.0026,  0.0221],\n",
       "           [-0.0069, -0.0116,  0.0356]],\n",
       " \n",
       "          [[ 0.0289,  0.0089, -0.0401],\n",
       "           [ 0.0266, -0.0418,  0.0281],\n",
       "           [ 0.0017, -0.0237,  0.0467]],\n",
       " \n",
       "          [[-0.0380, -0.0470,  0.0213],\n",
       "           [ 0.0005, -0.0104, -0.0239],\n",
       "           [-0.0278, -0.0101,  0.0372]]]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.0322, -0.0251,  0.0428, -0.0068, -0.0018, -0.0164, -0.0314,  0.0202,\n",
       "         -0.0180,  0.0061, -0.0246,  0.0293, -0.0267,  0.0238,  0.0138,  0.0129,\n",
       "         -0.0258,  0.0029, -0.0319,  0.0029], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0018,  0.0037,  0.0040,  ..., -0.0001,  0.0152, -0.0096],\n",
       "         [-0.0070,  0.0021,  0.0046,  ..., -0.0071, -0.0189,  0.0039],\n",
       "         [ 0.0056,  0.0048, -0.0012,  ..., -0.0134,  0.0141,  0.0073],\n",
       "         ...,\n",
       "         [ 0.0181, -0.0076, -0.0166,  ..., -0.0049,  0.0122, -0.0136],\n",
       "         [ 0.0022,  0.0202, -0.0135,  ...,  0.0044,  0.0070, -0.0145],\n",
       "         [ 0.0143,  0.0203,  0.0055,  ...,  0.0163,  0.0164, -0.0087]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.0026,  0.0166,  0.0170,  0.0166, -0.0191,  0.0131,  0.0142,  0.0017,\n",
       "         -0.0003, -0.0002, -0.0056,  0.0101, -0.0172, -0.0199, -0.0013,  0.0107,\n",
       "         -0.0189,  0.0144, -0.0103,  0.0130], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 2.1168e-01,  2.1643e-01, -1.9858e-01, -7.8331e-02, -1.3636e-01,\n",
       "           1.2177e-01, -8.6889e-02, -6.9155e-02,  7.5628e-03, -1.3484e-01,\n",
       "           1.2464e-01,  8.2791e-02, -8.0074e-02,  2.1521e-01,  1.5477e-02,\n",
       "          -2.1575e-01, -9.2560e-02, -2.2178e-01,  3.5049e-03,  3.7338e-02],\n",
       "         [-1.5551e-01, -3.0881e-02,  6.9663e-02, -8.0335e-02, -1.4055e-01,\n",
       "           1.0979e-01,  1.0017e-01,  2.1002e-01, -2.6420e-05, -2.2118e-01,\n",
       "          -3.0029e-02, -6.8850e-03, -1.9226e-02,  1.0669e-02, -6.5189e-02,\n",
       "          -1.4754e-01,  6.8961e-02,  2.2244e-01,  7.1523e-02, -9.3629e-02],\n",
       "         [-2.1451e-01,  8.3357e-02,  7.2310e-02,  8.8031e-02, -1.4183e-01,\n",
       "          -1.0624e-01,  3.8626e-02, -1.5212e-01,  1.4059e-01,  1.7347e-01,\n",
       "          -9.6296e-02,  1.3839e-01, -2.2194e-01, -1.8541e-01,  1.9360e-01,\n",
       "           1.5588e-01,  4.7554e-02, -1.4945e-01,  1.7218e-01,  1.3867e-01],\n",
       "         [-1.0353e-01,  3.5951e-02, -8.5088e-02, -2.2019e-01,  1.6122e-01,\n",
       "          -1.6784e-01,  2.1158e-01,  1.6946e-01, -3.1422e-02,  1.4727e-01,\n",
       "          -8.9346e-02, -1.2560e-01,  1.2059e-01,  6.8884e-02,  1.6228e-01,\n",
       "          -1.7188e-01,  1.4222e-01,  5.9474e-02,  1.1767e-03,  5.1864e-02],\n",
       "         [-9.0050e-02, -5.4442e-02, -2.2330e-02, -2.4757e-02, -1.4479e-01,\n",
       "           2.1783e-01,  8.8640e-03, -1.3200e-01, -3.6511e-04, -1.5910e-01,\n",
       "          -1.0522e-01,  5.0782e-02,  5.9173e-02,  1.7164e-02, -2.9144e-04,\n",
       "          -1.8175e-01, -2.1932e-03,  1.7530e-02, -1.5482e-01, -1.5750e-01],\n",
       "         [ 1.1594e-01, -1.4988e-01,  4.6555e-02, -1.6821e-01,  2.3140e-02,\n",
       "          -1.3952e-01,  1.9754e-01,  1.2983e-01,  8.6760e-02, -1.1822e-01,\n",
       "           1.1939e-01, -1.7053e-01, -1.5331e-01,  1.6518e-01,  1.6908e-01,\n",
       "          -1.6435e-01,  7.5341e-02, -6.9287e-02,  1.2124e-01, -2.1680e-01],\n",
       "         [-5.0042e-02, -1.5179e-01, -3.5238e-02, -7.4435e-02, -1.5465e-01,\n",
       "           4.7882e-02, -9.5838e-02,  1.6627e-01, -4.9898e-02, -4.4477e-02,\n",
       "          -1.2137e-01, -9.8775e-02, -9.7129e-03,  1.5293e-01,  2.1390e-01,\n",
       "           2.2797e-02, -2.0440e-01,  1.7891e-01, -7.3664e-02,  1.8570e-01],\n",
       "         [-4.2663e-02,  7.8480e-02, -1.6143e-02, -1.4911e-01,  1.1282e-01,\n",
       "          -1.9807e-01, -2.0381e-01,  1.1119e-01, -5.2908e-02,  9.6745e-02,\n",
       "          -4.0722e-02, -9.9553e-02, -8.4288e-02, -2.0460e-01,  7.8671e-02,\n",
       "          -3.8498e-02,  3.6114e-02, -2.1144e-01,  1.3960e-01, -1.6831e-01],\n",
       "         [ 1.9242e-01, -5.2277e-02,  8.1309e-02,  7.6503e-02,  1.5183e-01,\n",
       "          -5.2880e-02,  2.1168e-01, -1.7500e-01,  1.6972e-01, -1.8221e-01,\n",
       "          -7.6728e-02,  1.0921e-01, -6.8398e-02, -4.1794e-03, -4.0958e-02,\n",
       "          -1.8757e-01,  2.0924e-01,  1.3051e-01, -1.5796e-01,  2.0590e-02],\n",
       "         [-1.7970e-01,  8.9557e-02,  5.8485e-02, -1.1601e-01,  1.3468e-01,\n",
       "           8.6942e-02,  2.0084e-01,  1.2306e-01, -8.2925e-02, -2.5852e-02,\n",
       "           1.2568e-01,  1.0224e-01,  1.2190e-01, -9.3200e-02,  1.8028e-01,\n",
       "          -1.4675e-01, -2.6966e-03,  1.3772e-01,  1.0191e-01, -3.5359e-02]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.1059, -0.1779, -0.0750, -0.0936, -0.1171, -0.1558, -0.0292,  0.1386,\n",
       "          0.1984,  0.2062], requires_grad=True)]"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(conv_layers.parameters()) + list(fc_layers.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(conv_layers, fc_layers, X, T, n_epochs, learning_rate):\n",
    "    \n",
    "    optimizer = torch.optim.Adam(list(conv_layers.parameters()) + list(fc_layers.parameters()),\n",
    "                                 lr=learning_rate)\n",
    "    loss_func = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "    \n",
    "        Y = forward(conv_layers, fc_layers, X)\n",
    "        \n",
    "        loss = loss_func(Y[-1], T)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step() \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        pc_train = percent_correct(use(conv_layers, fc_layers, Xtrain), Ttrain)\n",
    "        pc_val = percent_correct(use(conv_layers, fc_layers, Xval), Tval)\n",
    "        pc_test = percent_correct(use(conv_layers, fc_layers, Xtest), Ttest)\n",
    "        \n",
    "        print(f'Epoch {epoch + 1} %correct: Train {pc_train:.1f} Val {pc_val:.1f} Test {pc_test:.1f}')\n",
    "\n",
    "def use(conv_layers, fc_layers, X):\n",
    "    Y = forward(conv_layers, fc_layers, X)\n",
    "    class_index = torch.argmax(Y[-1], axis=1) \n",
    "    return class_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 %correct: Train 45.7 Val 47.2 Test 47.8\n",
      "Epoch 2 %correct: Train 65.7 Val 67.5 Test 67.4\n",
      "Epoch 3 %correct: Train 71.4 Val 73.9 Test 73.0\n",
      "Epoch 4 %correct: Train 66.7 Val 69.6 Test 68.0\n",
      "Epoch 5 %correct: Train 63.0 Val 65.7 Test 64.3\n",
      "Epoch 6 %correct: Train 63.4 Val 66.2 Test 64.8\n",
      "Epoch 7 %correct: Train 66.9 Val 69.3 Test 68.5\n",
      "Epoch 8 %correct: Train 71.0 Val 73.2 Test 72.5\n",
      "Epoch 9 %correct: Train 73.9 Val 76.3 Test 75.2\n",
      "Epoch 10 %correct: Train 75.8 Val 78.2 Test 77.0\n"
     ]
    }
   ],
   "source": [
    "train(conv_layers, fc_layers, Xtrain, Ttrain, 10, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 8, 6,  ..., 5, 6, 8])"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use(conv_layers, fc_layers, Xval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
