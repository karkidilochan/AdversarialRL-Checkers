# Adversarial Reinforcement Learning for Checkers Game

The game can be modeled by a 8x8 matrix with a value representing each player as content of a cell. However, since the pieces move only diagonally along the black cells of a checkers board, the board can be modeled with a compressed version with a 32x1 array by removing those unreachable cells. For this project, I will be using the functions that model the pieces and board, and define the rules for them from [Btsan-checkersbot]. In the case of reinforcement learning, the player will be the agent, positios of the king and men pieces would be the states and actions would be the moves made on the pieces and capturing opponent pieces. The outcomes of these actions at each state would be defined as rewards which would be positive if the player is closer to winning, else negative. The state-action function, also defined as Q-function, to predict the expected sum of rewards could be modeled as a table, but considering the large number of possible game states and actions, this would be infeasible. Thus, a neural network can be used for the approximation of the Q-function. It will take the board positions and actions taken along with a metric to measure the chances of winning for a given state-action value.  

Generative models are a type of statistical models that create new examples of data [Goodfellow-Generative]. In unsupervised learning, these models help with tasks like figuring out how likely something is to happen, describing data points, and sorting things into groups based on these probabilities. Its goal is to study training samples and learn the probability distribution that generated them. Then, using this model, more examples are generated which are then used to train the agent. With this adversarial process, I will be training two models, a generative model called `gen_model` and a discriminative model called `board_model`. The generative model will capture the data distribution and the discriminate model will estimate the probability that the board position metric came from the training data and not the generative model. The model will be trained for multiple generations with the goal of maximizing the chances that the discriminative model will make a mistake.

From the results, it can be seen that deep learning with generative adversarial networks can be used to develop a reinforced agent that is capable of playing Checkers with a reasonable winrate of approximately 80%. This was done by first creating a generative model fitted on the metrics from a heuristics function. Using this model, a discriminative model that evaluated a board position was created and reinforced through multiple generations with each generation having the agent play through multiple games. This approach seems more efficient than using Q-tables for the state-action pair, or using neural network approximations for a Q-function on Markov processes that we learned in the classes. Tweaking the hyperparameters of the adversarial networks, the chances of the agent improving in each game and generation can be increased. In the first few iterations, the agent learns very quickly, but after a while, it stabilizes and gives steady performance. This might mean that the agent is capable of learning most of the board positions, but as it plays more games in each generation, it explores more board positions that are difficult to beat. 

This also shows that simply increasing the number of iterations, or tweaking the hyperparameters doesn't ensure improvement in performance. More experiments in the architecture of the adversarial networks or better methods to evaluate the board position might be needed to create a stronger agent. It was hard to figure out an optimal set of weights for the metrics function besides the ones used in the references. It was also difficult to determine set of hyperparameters and the network architecture for optimal performance as training times were significantly high, taking about 1 hour to train an agent through 100 generations with 100 games played in each generation. To obtain good results, the architecture and most of the hyperparameteres used for the adversarial network were similar to the ones in refered sources. 

Based on the works like [Henning,RL-Checkers], I was expecting a exponential and steady rise in the winrate and steady performance without any significant drops in the performance. But, as observed in the plots above, the model winrates would peak early and drop significantly. It was difficult to figure out what parameter was responsible for this. I experimented with lower learning rates to see if the fluctuations would get reduced but was getting similar results. It would have been interesting to explore more with various set of parameters and see their results but the training times for larger generatios were significantly high. 
 
